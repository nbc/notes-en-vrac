[
  {
    "objectID": "introduction-duckdb.html",
    "href": "introduction-duckdb.html",
    "title": "Traiter des données massives en R avec duckdb",
    "section": "",
    "text": "L’objectif de ce document est de montrer comment utiliser duckdb, il fait suite au document d’introduction sur arrow.\nNous commençons par charger quelques packages utiles :"
  },
  {
    "objectID": "introduction-duckdb.html#duckdb-quest-ce-que-cest",
    "href": "introduction-duckdb.html#duckdb-quest-ce-que-cest",
    "title": "Traiter des données massives en R avec duckdb",
    "section": "duckdb qu’est-ce que c’est ?",
    "text": "duckdb qu’est-ce que c’est ?\nduckdb est une base de données embarquée :\n\nopen source\norientée traitement analytique\ntrès (très) rapide\nrequêtable avec dplyr/dbplyr ou directement en SQL\nqui sait lire les fichiers parquet et interagir avec arrow\nqui ne demande (quasiment) aucun paramétrage\nqui tourne à l’intérieur de votre R et n’est accessible que par vous\nqui propose une syntaxe SQL “enrichie” (PIVOT, SAMPLE, SELECT * EXCLUDE…)\n\nduckdb étant une base de données, les packages DBI (R DataBase Interface) et dbplyr (dplyr pour les Bases de données) seront utilisés et rapidement décrits, pour plus d’informations, vous pouvez vous reporter à la documentation utilitr : se connecter à une base de données.\nEn résumé :\n\nduckdb est une base de données\nDBI permet de se connecter sur une base de données et de lancer des requêtes\ndbplyr permet d’utiliser les fonctions de dplyr et les transforme en requêtes SQL\n\nLes exemples de la suite sont en R mais duckdb sait aussi travailler avec python et plein d’autres langages\n\n\n\n\n\n\nQuelques éléments sur l’accès aux bases de données en R\n\n\n\nDBI est le package qui permet d’interagir avec les bases de données. Les commandes principales sont :\n\nDBI::dbConnect() : pour se connecter à une base de données\nDBI::dbGetQuery() : exécute une requête et retourne le résultat (pour les requêtes de type SELECT)\nDBI::dbExecute() : exécute une requête sans retourner le résultat (pour les requêtes de type CREATE)\nDBI::dbDdisconnect() : pour se déconnecter d’une base de données\n\nPour plus d’informations, allez voir la page d’utilitr sur le sujet ou la documentation de DBI"
  },
  {
    "objectID": "introduction-duckdb.html#petit-retour-vers-arrow",
    "href": "introduction-duckdb.html#petit-retour-vers-arrow",
    "title": "Traiter des données massives en R avec duckdb",
    "section": "Petit retour vers arrow",
    "text": "Petit retour vers arrow\nPour commencer nous allons ressortir le jeu de données des courses de taxi new-yorkais de 2009 à 2023 :\n\ntaxi_dir <- \"/nfs/partage-r-sas/exemples/taxis-data/nyc-taxi\"\n\n\n\n\n\nopen_dataset(taxi_dir)\n\nFileSystemDataset with 158 Parquet files\nvendor_name: string\npickup_datetime: timestamp[ms]\ndropoff_datetime: timestamp[ms]\npassenger_count: int64\ntrip_distance: double\npickup_longitude: double\npickup_latitude: double\nrate_code: string\nstore_and_fwd: string\ndropoff_longitude: double\ndropoff_latitude: double\npayment_type: string\nfare_amount: double\nextra: double\nmta_tax: double\ntip_amount: double\ntolls_amount: double\ntotal_amount: double\nimprovement_surcharge: double\ncongestion_surcharge: double\npickup_location_id: int64\ndropoff_location_id: int64\nyear: int32\nmonth: int32\n\nlist.files(taxi_dir)\n\n [1] \"year=2009\" \"year=2010\" \"year=2011\" \"year=2012\" \"year=2013\" \"year=2014\"\n [7] \"year=2015\" \"year=2016\" \"year=2017\" \"year=2018\" \"year=2019\" \"year=2020\"\n[13] \"year=2021\" \"year=2022\"\n\nopen_dataset(taxi_dir) |> nrow()\n\n[1] 1672590319\n\n\nNous avons vu dans la session sur arrow que la fonction to_duckdb() permettait de passer des données de arrow à duckdb sans passer par un data frame/tibble intermédiaire.\nReprenons rapidement cet exemple en cherchant la course avec la distance la plus longue sur janvier 2018 :\n\nopen_dataset(taxi_dir) |>\n  filter(year == 2018 & month == 1) |>\n  filter(trip_distance == max(trip_distance, na.rm = TRUE)) |>\n  collect()\n\nError: Filter expression not supported for Arrow Datasets: trip_distance == max(trip_distance, na.rm = TRUE)\nCall collect() first to pull data into R.\n\n\narrow ne supporte pas (encore ?) le filtre utilisé et nous propose de faire le collect(), donc de récupérer l’ensemble des données en mémoire, avant le filtre, ce qui serait une très mauvaise idée même sur un seul mois de courses…\nEssayons plutôt d’utiliser to_duckdb() :\n\ntic()\nopen_dataset(taxi_dir) |>\n  filter(year == 2018 & month == 1) |>\n  to_duckdb() |>\n  filter(trip_distance == max(trip_distance, na.rm = TRUE)) |>\n  collect()\n\n# A tibble: 1 × 24\n  vendor_name pickup_datetime     dropoff_datetime    passenger_count\n  <chr>       <dttm>              <dttm>                        <dbl>\n1 VTS         2018-01-30 11:41:02 2018-01-30 11:42:09               1\n# ℹ 20 more variables: trip_distance <dbl>, pickup_longitude <dbl>,\n#   pickup_latitude <dbl>, rate_code <chr>, store_and_fwd <chr>,\n#   dropoff_longitude <dbl>, dropoff_latitude <dbl>, payment_type <chr>,\n#   fare_amount <dbl>, extra <dbl>, mta_tax <dbl>, tip_amount <dbl>,\n#   tolls_amount <dbl>, total_amount <dbl>, improvement_surcharge <dbl>,\n#   congestion_surcharge <dbl>, pickup_location_id <dbl>,\n#   dropoff_location_id <dbl>, year <int>, month <int>\n\ntoc()\n\n7.807 sec elapsed\n\n\nduckdb sait gérer le filtre utilisé et arrow::to_duckdb() ne matérialise pas les données : il convertit un objet arrow au format duckdb et le passe à duckdb pour réaliser les calculs.\nNous venons de trouver la ligne avec la distance de course la plus grande parmi des millions de lignes en quelques secondes.\nPour être complet, même si dans le cas présent cela n’a aucun intérêt, on peut également réaliser l’opération inverse avec arrow::to_arrow() qui reconvertit un objet de duckdb vers arrow :\n\nopen_dataset(taxi_dir) |>\n  filter(year == 2018 & month == 1) |>\n  to_duckdb() |>\n  filter(trip_distance == max(trip_distance, na.rm = TRUE)) |>\n  to_arrow() |>\n  collect()\n\n# A tibble: 1 × 24\n  vendor_name pickup_datetime     dropoff_datetime    passenger_count\n  <chr>       <dttm>              <dttm>                        <int>\n1 VTS         2018-01-30 12:41:02 2018-01-30 12:42:09               1\n# ℹ 20 more variables: trip_distance <dbl>, pickup_longitude <dbl>,\n#   pickup_latitude <dbl>, rate_code <chr>, store_and_fwd <chr>,\n#   dropoff_longitude <dbl>, dropoff_latitude <dbl>, payment_type <chr>,\n#   fare_amount <dbl>, extra <dbl>, mta_tax <dbl>, tip_amount <dbl>,\n#   tolls_amount <dbl>, total_amount <dbl>, improvement_surcharge <dbl>,\n#   congestion_surcharge <dbl>, pickup_location_id <int>,\n#   dropoff_location_id <int>, year <int>, month <int>\n\n\nÀ ce jour, arrow ne supporte pas les fonctions de fenêtre mais passer par duckdb ne pose aucun problème.\nPasser de arrow à duckdb et vice versa fonctionne parfaitement et est très pratique par exemple quand une fonction n’existe pas dans arrow mais n’est pas optimal et peut poser des problèmes sur des gros volumes de données."
  },
  {
    "objectID": "introduction-duckdb.html#utiliser-duckdb-nativement",
    "href": "introduction-duckdb.html#utiliser-duckdb-nativement",
    "title": "Traiter des données massives en R avec duckdb",
    "section": "Utiliser duckdb nativement",
    "text": "Utiliser duckdb nativement\nduckdb est une base de données, pour l’utiliser il faut :\n\ncréer sa base de données\nde se connecter dessus.\n\nCeux qui connaissent les “vraies” bases de données savent que ça n’est pas simple mais…\nCommençons par créer une base de données duckdb et ouvrir une connexion qui nous permettra de la manipuler :\n\ncon <- DBI::dbConnect(duckdb())\n\nEt c’est fini. Nous avons créé une base de données vide anonyme (sans nom et en mémoire) et ouvert une connexion dessus, celle-ci étant disponible dans la variable con.\n\n\n\n\n\n\ngérer les limites des serveurs\n\n\n\nEn pratique, il est fortement conseillé de fixer des limites à duckdb en utilisant une chaîne de connexion du type :\n\ncon <- dbConnect(\n  duckdb(), \n  dbdir = \"tuto_duckdb.db\", \n  config = list(\n    memory_limit = \"10G\",\n    threads = \"4\")\n  )\n\nCette chaîne de connexion va créer une base de données stockée dans un fichier tuto_duckdb.db avec des limites à 10Go de mémoire vive (à adapter à votre quota mémoire) et 4 cœurs. Si duckdb commence à atteindre son quota de mémoire vive, il passera par des fichiers temporaires.\nEn utilisant cette connexion, vous vous assurez que vous ne saturerez pas les processeurs serveurs et que vos traitements ne planteront pas faute de mémoire.\nSur les serveurs actuels, nous recommandons de ne pas dépasser 4 threads, cette directive évoluera avec l’arrivée des futurs serveurs qui disposeront de beaucoup plus de coeurs.\n\n\nOn utilise ensuite la fonction tbl() qui va permettre de rendre accessible des données au format parquet (ou autre, nous le verrons plus tard) dans la base de données créée.\n\n\n\n\n\n\nNe mettez pas vos chemins en dur dans votre code\n\n\n\nPlutôt que de répéter partout dans votre code le chemin vers vos fichiers, vous pouvez utiliser glue::glue() qui vient avec le tidyverse. Cette commande va remplacer les chaines entre parenthèse par le contenu de la variable :\n\nglue::glue(\"read_parquet('{taxi_dir}/**/*.parquet')\")\n\nread_parquet('/nfs/partage-r-sas/exemples/taxis-data/nyc-taxi/**/*.parquet')\n\n\n\n\n\ntaxi <- tbl(con, glue(\"read_parquet('{taxi_dir}/**/*.parquet')\"))\n\ndplyr::tbl() va lancer l’ordre passé en deuxième argument, à savoir read_parquet(...) (rien à voir avec arrow::read_parquet()) dans la base de données précédemment créée et cette fonction va mettre à disposition les données des fichiers parquet comme si elles étaient vraiment stockées dans la base de données.\nUn point important : contrairement à arrow::open_dataset() qui prend en argument un répertoire, la fonction read_parquet() de duckdb prend des chemins de fichiers, dans le cas présent, le **/*.parquet veut dire : tous les fichiers avec une extension .parquet présents dans les sous-répertoires de taxi_dir. duckdb saura reconnaître les partitions s’il voit des répertoires nommées dans un motif variable=valeur.\nNous pouvons visualiser la variable taxi :\n\ntaxi\n\n# Source:   table<read_parquet('/nfs/partage-r-sas/exemples/taxis-data/nyc-taxi/**/*.parquet')> [?? x 24]\n# Database: DuckDB v0.9.1 [unknown@Linux 5.4.0-156-generic:R 4.3.1/tuto_duckdb.db]\n   vendor_name pickup_datetime     dropoff_datetime    passenger_count\n   <chr>       <dttm>              <dttm>                        <dbl>\n 1 VTS         2019-04-01 08:20:31 2019-04-01 08:35:32               2\n 2 VTS         2019-04-01 08:36:22 2019-04-01 08:37:56               2\n 3 VTS         2019-04-01 08:39:59 2019-04-01 08:45:40               2\n 4 VTS         2019-04-01 08:50:58 2019-04-01 09:20:11               2\n 5 VTS         2019-04-01 08:11:16 2019-04-01 08:12:56               1\n 6 VTS         2019-04-01 08:14:42 2019-04-01 08:20:59               1\n 7 VTS         2019-04-01 08:06:10 2019-04-01 08:16:46               1\n 8 VTS         2019-04-01 08:20:20 2019-04-01 08:29:20               1\n 9 VTS         2019-04-01 08:30:02 2019-04-01 08:41:32               1\n10 VTS         2019-04-01 08:42:17 2019-04-01 08:45:13               1\n# ℹ more rows\n# ℹ 20 more variables: trip_distance <dbl>, pickup_longitude <dbl>,\n#   pickup_latitude <dbl>, rate_code <chr>, store_and_fwd <chr>,\n#   dropoff_longitude <dbl>, dropoff_latitude <dbl>, payment_type <chr>,\n#   fare_amount <dbl>, extra <dbl>, mta_tax <dbl>, tip_amount <dbl>,\n#   tolls_amount <dbl>, total_amount <dbl>, improvement_surcharge <dbl>,\n#   congestion_surcharge <dbl>, pickup_location_id <dbl>, …\n\n\ndont le contenu ressemble à un dataframe classique mais les premières lignes nous indique qu’il s’agit bien de données stockées dans une base de données dont la source est un dataset parquet.\nFaisons quelques manipulations, nous voulons la distance des courses en km et ajouter une colonne avec le jour de la semaine de la course. dbplyr comprends beaucoup d’ordres du tidyverse et lubridate::wday() en fait partie :\n\ntic()\ncalcul <- taxi |> \n  filter(year == 2016) |>\n  mutate(wday = wday(pickup_datetime),\n         trip_distance = trip_distance * 1.61)\n\ncalcul |>\n  select(wday, trip_distance)\n\n# Source:   SQL [?? x 2]\n# Database: DuckDB 0.9.0 [unknown@Linux 6.2.0-36-generic:R 4.3.2/tuto_duckdb.db]\n    wday trip_distance\n   <dbl>         <dbl>\n 1     2         1.13 \n 2     2         1.29 \n 3     2         4.93 \n 4     2         0.644\n 5     2         3.74 \n 6     2         3.70 \n 7     2         5.94 \n 8     2         1.13 \n 9     2         2.48 \n10     2         2.58 \n# ℹ more rows\n\ntoc()\n\n0.099 sec elapsed\n\n\nCe calcul devrait retourner plusieurs millions de lignes mais s’exécute en quelques ms. Que se passe-t-il ?\n\n\n\n\n\n\ndbplyr et l’évaluation fainéante\n\n\n\nContrairement à dplyr qui traite les ordres un par un, dbplyr va analyser l’ensemble de vos ordres, les transformer en SQL et l’envoyer à la base de données pour exécution mais de façon très particulière :\n\nil récupère les données uniquement si vous lui demandez explicitement\nil retarde l’exécution des requêtes le plus possible, quand vous demandez le résultat d’un calcul.\nsi vous “prévisualiser” un calcul, il va récupérer seulement quelques lignes\n\nPour lui demander explicitement le résultat d’une requête, vous devez utiliser les fonctions :\n\ncollect() qui va récupérer les données en mémoire sous la forme d’un tibble\ncompute() qui va créer une table temporaire dans votre base, table que vous pourrez utiliser comme un tibble."
  },
  {
    "objectID": "introduction-duckdb.html#fichiers-externes-et-jointures",
    "href": "introduction-duckdb.html#fichiers-externes-et-jointures",
    "title": "Traiter des données massives en R avec duckdb",
    "section": "fichiers externes et jointures",
    "text": "fichiers externes et jointures\nDans la suite, nous voudrions étudier les courses de taxi partant des 3 aéroports de New York, le problème est que l’information n’est pas présente dans les données actuelles. Heureusement, le jeu de données contient une colonne pickup_location_id et nous avons la correspondance dans le fichier taxi+_zone_lookup.csv se trouvant dans le répertoire parent de notre dataset sur les courses :\n\nreadr::read_csv(file.path(taxi_dir, \"../taxi+_zone_lookup.csv\")) |>\n  filter(if_any(everything(), ~str_detect(., \"(?i)airport\")))\n\n# A tibble: 3 × 4\n  LocationID Borough Zone              service_zone\n       <dbl> <chr>   <chr>             <chr>       \n1          1 EWR     Newark Airport    EWR         \n2        132 Queens  JFK Airport       Airports    \n3        138 Queens  LaGuardia Airport Airports    \n\n\nEt nous voyons bien les aéroports New Yorkais ! Visiblement les colonnes intéressantes sont LocationID et Zones. Nous allons les sélectionner et normaliser les noms :\n\nzones <- readr::read_csv(file.path(taxi_dir, \"../taxi+_zone_lookup.csv\")) |>\n  select(\n    pickup_location_id = LocationID,\n    pickup_zone = Zone\n  )\nzones\n\n# A tibble: 265 × 2\n   pickup_location_id pickup_zone            \n                <dbl> <chr>                  \n 1                  1 Newark Airport         \n 2                  2 Jamaica Bay            \n 3                  3 Allerton/Pelham Gardens\n 4                  4 Alphabet City          \n 5                  5 Arden Heights          \n 6                  6 Arrochar/Fort Wadsworth\n 7                  7 Astoria                \n 8                  8 Astoria Park           \n 9                  9 Auburndale             \n10                 10 Baisley Park           \n# ℹ 255 more rows\n\n\nNous devons maintenant mettre à disposition ces données dans la base duckdb, il y a plusieurs façons de le faire mais nous allons en voir deux qui sont sans doute les plus flexibles :\n\nDBI::copy_to() : qui copie les données d’un dataframe dans la base\nduckdb::duckdb_register() qui enregistre les données d’un dataframe comme une table virtuelle sans les copier.\n\nNous aurions aussi pu utiliser les fonctions d’import CSV de duckdb de duckdb qui sont très efficaces mais elles sont pour le moment limitées au CSV en UTF8 (ce qui est problématique pour beaucoup de vos données) et peuvent poser des problèmes sur des fichiers malformés. Si vous devez traiter des fichiers CSV volumineux (ou qui vous posent problème avec les outils classiques), vous devrez d’abord convertir vos fichiers en UTF8 (passez nous voir, nous pouvons vous aider).\n\ndplyr::copy_to()\n\ncopy_to(con, zones, \"zones\", overwrite = TRUE)\n\nCette commande va copier dans la base pointée par con le dataframe passées en deuxième argument zones dans une table ayant le nom du troisième argument (et pour éviter les problèmes à la génération de cette documentation j’autorise l’écrasement des données.)\n\n\nduckdb::duckdb_register()\n\nduckdb::duckdb_register(con, \"zones_duckdb\", zones)\n\nCette commande va “enregistrer” les données du troisième argument dans la table nommée par le deuxième argument. Aucune écriture de données n’est réalisée.\n\n\n\n\n\n\nAstuce\n\n\n\nSi vos données externes (CSV ou autres) sont volumineuses, vous avez intérêt à les convertir en parquet une fois pour toute pour bénéficier de toute la puissance de duckdb plutôt que de les copier ou les enregistrer à chaque fois.\n\n\n\n\nLa jointure\nMaintenant que nos zones sont mises à disposition dans duckdb, nous pouvons y accéder en utilisant tbl() qui peut, évidemment, mettre à disposition une table existante :\n\ntbl(con, \"zones\")\n\n# Source:   table<zones> [?? x 2]\n# Database: DuckDB v0.9.1 [unknown@Linux 5.4.0-156-generic:R 4.3.1/tuto_duckdb.db]\n   pickup_location_id pickup_zone            \n                <dbl> <chr>                  \n 1                  1 Newark Airport         \n 2                  2 Jamaica Bay            \n 3                  3 Allerton/Pelham Gardens\n 4                  4 Alphabet City          \n 5                  5 Arden Heights          \n 6                  6 Arrochar/Fort Wadsworth\n 7                  7 Astoria                \n 8                  8 Astoria Park           \n 9                  9 Auburndale             \n10                 10 Baisley Park           \n# ℹ more rows\n\n\nEt nous pouvons lancer notre requête pour trouver les distances moyennes des courses par jour de la semaine sur l’année 2016 en filtrant les lignes contenant la chaine airport dans la colonne pickup_zone :\n\ntic()\ntbl(con, glue(\"{taxi_dir}/**/*.parquet\")) |>\n  filter(year == 2016) |>\n  mutate(wday = wday(pickup_datetime)) |>\n  left_join(tbl(con, \"zones\"))|>\n  # (?i) est pour indiquer que la recherche doit chercher toutes les casses\n  # majuscule et minuscule\n  filter(str_detect(pickup_zone, \"(?i)airport\")) |>\n  group_by(pickup_zone, wday) |>\n  summarize(mean_trip = mean(trip_distance)) |>\n  arrange(wday) |>\n  collect() |>\n  pivot_wider(names_from = wday, values_from = mean_trip)\n\n# A tibble: 3 × 8\n# Groups:   pickup_zone [3]\n  pickup_zone         `1`   `2`   `3`   `4`   `5`   `6`   `7`\n  <chr>             <dbl> <dbl> <dbl> <dbl> <dbl> <dbl> <dbl>\n1 LaGuardia Airport  9.87  9.94 10.0  10.1  10.0   9.90  9.82\n2 JFK Airport       18.7  48.9  17.3  15.7  17.3  25.0  15.8 \n3 Newark Airport     2.25  2.09  2.24  2.00  1.93  2.44  2.49\n\ntoc()\n\n5.223 sec elapsed\n\n\ndbplyr reconnait énormément de fonctions du tidyverse que ça soit stringr, lubridate ou autre.\n“Et voila”. Pas mal pour un traitement sur plus de 1,6 milliard de lignes et 64Go de volume…\n\n\n\n\n\n\nNote\n\n\n\nduckdb analyse la requête dans sa globalité et l’optimise, une des optimisations est au niveau de la lecture en s’appuyant sur le format parquet qui est partitionné et organisé en colonnes.\nDonc duckdb va lire :\n\nuniquement les fichiers présent dans le répertoire year=2016\njuste les colonnes nécessaires, à savoir pickup_datetime pour calculer le jour de la semaine, trip_distance pour la moyenne et bien sûr pickup_location_id pour faire la jointure.\n\nD’autres optimisations sont réalisées grâce aux métadonnées stockées dans les fichiers parquet mais restons simple."
  },
  {
    "objectID": "introduction-duckdb.html#petite-comparaison-avec-arrow",
    "href": "introduction-duckdb.html#petite-comparaison-avec-arrow",
    "title": "Traiter des données massives en R avec duckdb",
    "section": "Petite comparaison avec arrow",
    "text": "Petite comparaison avec arrow\nComparons rapidement avec l’implémentation en arrow. Le premier jet ressemble beaucoup à ce que nous avons fait précédemment, sachant que arrow sait convertir à la volée des données d’un dataframe en objet arrow nous allons juste faire :\n\nzones <- readr::read_csv(file.path(taxi_dir, \"../taxi+_zone_lookup.csv\")) |>\n  select(\n    pickup_location_id = LocationID,\n    pickup_zone = Zone\n  )\n\nopen_dataset(taxi_dir) |>\n  filter(year == 2016) |>\n  mutate(wday = wday(pickup_datetime)) |>\n  left_join(zones) |>\n  filter(str_detect(pickup_zone, \"(?i)airport\")) |>\n  group_by(pickup_zone, wday) |>\n  summarize(mean_trip = mean(trip_distance, na.rm = TRUE)) |>\n  arrange(wday) |>\n  collect() |>\n  pivot_wider(names_from = wday, values_from = mean_trip)\n\nError in `compute.arrow_dplyr_query()`:\n! Invalid: Incompatible data types for corresponding join field keys: FieldRef.Name(pickup_location_id) of type int64 and FieldRef.Name(pickup_location_id) of type double\n\n\nHa oui, nous avons vu ce problème dans la session sur arrow, celui-ci est très à cheval sur les types des données utilisées dans les jointures, qu’à cela ne tienne, précisons le type de la colonne pickup_location_id et relançons la requête :\n\nzones_arrow <- readr::read_csv(file.path(taxi_dir, \"../taxi+_zone_lookup.csv\")) |>\n  select(\n    pickup_location_id = LocationID,\n    pickup_zone = Zone\n  ) |>\n  as_arrow_table(\n    schema = schema(\n      pickup_location_id = int64(),\n      pickup_zone = utf8()\n    )\n  )\n\ntic()\nopen_dataset(taxi_dir) |>\n  filter(year == 2016) |>\n  mutate(wday = wday(pickup_datetime)) |>\n  left_join(zones_arrow) |>\n  filter(str_detect(pickup_zone, \"(?i)airport\")) |>\n  group_by(pickup_zone, wday) |>\n  summarize(mean_trip = mean(trip_distance, na.rm = TRUE)) |>\n  arrange(wday) |>\n  collect() |>\n  pivot_wider(names_from = wday, values_from = mean_trip)\n\n# A tibble: 3 × 8\n# Groups:   pickup_zone [3]\n  pickup_zone         `1`   `2`   `3`   `4`   `5`   `6`   `7`\n  <chr>             <dbl> <dbl> <dbl> <dbl> <dbl> <dbl> <dbl>\n1 LaGuardia Airport  9.87  9.94 10.0  10.1  10.0   9.90  9.82\n2 JFK Airport       18.7  48.9  17.3  15.7  17.3  25.0  15.8 \n3 Newark Airport     2.25  2.09  2.24  2.00  1.93  2.44  2.49\n\ntoc()\n\n11.921 sec elapsed\n\n\nNous obtenons bien le même résultat mais, dans le cas présent, arrow est beaucoup plus lent que duckdb (et arrow est moins souple sur les types pour les jointures)\n\n\n\n\n\n\nduckdb est très fort en jointure\n\n\n\nQuand vous avez des jointures entre des tables “volumineuses”, privilégiez duckdb"
  },
  {
    "objectID": "introduction-duckdb.html#générer-des-fichiers-intermédiaires-avec-duckdb",
    "href": "introduction-duckdb.html#générer-des-fichiers-intermédiaires-avec-duckdb",
    "title": "Traiter des données massives en R avec duckdb",
    "section": "générer des fichiers intermédiaires avec duckdb",
    "text": "générer des fichiers intermédiaires avec duckdb\nQue ça soit dans la phase de nettoyage des sources ou dans la phase de calcul finale, il est parfois nécessaire de passer par des fichiers intermédiaires.\nTraditionnellement, on charge les données en mémoire, on les manipule (en mémoire) et on les écrit dans un nouveau fichier. Cette méthode atteint vite ses limites quand on travaille sur des fichiers de plusieurs dizaines voire centaines de gigaoctet tant par les temps de chargement que par la mémoire nécessaire.\nNous allons prendre un fichier intermédiaire très simple :\n\nresult <- tbl(con, glue(\"{taxi_dir}/**/*.parquet\")) |>\n  filter(year == 2016) |>\n  mutate(wday = wday(pickup_datetime)) |>\n  left_join(tbl(con, \"zones\"))\n\nJoining with `by = join_by(pickup_location_id)`\n\n\nLes données font un peu plus de 100 millions de lignes et les fichiers parquets autour de 2,5Go. Pour avoir un ordre de grandeur, le dataframe fait de l’ordre de 28Go en mémoire.\n\nPour les fichiers pérennes : parquet\nSi vos fichiers intermédiaires sont pérennes et volumineux, il est recommandé de les générer au format parquet.\nIl y a trois façons de faire ayant des résultats différents en couplant tbl() et les fonctions arrow::write_parquet() et arrow::write_dataset() :\n\nresult |>\n  collect() |>\n  write_parquet(\"fichier_intermediaire_1.parquet\")\n\n\nresult |>\n  to_arrow() |>\n  write_parquet(\"fichier_intermediaire_2.parquet\")\n\n\nresult |>\n  to_arrow() |>\n  write_dataset(\"fichier_intermediaire\")\n\n\n\n\nnum\nméthode\nmémoire\n\n\n\n\n1\ntbl() |> collect() |> write_parquet()\n50Go\n\n\n2\ntbl() |> to_arrow() |> write_parquet()\n25Go\n\n\n3\ntbl() |> to_arrow() |> write_dataset()\n15Go\n\n\n\nToutes ces méthodes ont un inconvénient majeur, elle nécessite de passer de duckdb à arrow et doivent matérialiser plus ou moins de données (soit dans un dataframe, soit dans un objet arrow).\n\nmatérialise l’ensemble des données en mémoire sous la forme d’un dataframe\nmatérialise l’ensemble des données en mémoire sous la forme d’un objet arrow\nmatérialise les données par morceaux sous forme d’objet arrow\n\nduckdb a une commande COPY qui permet d’exporter des données de la base de la forme : COPY {from} TO {to} ({config}).\nPour copier vers un fichier parquet nous pouvons putiliser le sql généré par la requête et pour ça il existe une fonction pratique DBI::SQL() qui permet de le réutiliser. Nous allons donc stocker la requête :\n\nrequest <- result |>\n  dbplyr::sql_render(con = con)\nrequest\n\n<SQL> SELECT LHS.*, pickup_zone\nFROM (\n  SELECT *, EXTRACT('dow' FROM CAST(pickup_datetime AS DATE) + 0) + 1 AS wday\n  FROM \"/nfs/partage-r-sas/exemples/taxis-data/nyc-taxi/**/*.parquet\"\n  WHERE (\"year\" = 2016.0)\n) LHS\nLEFT JOIN zones\n  ON (LHS.pickup_location_id = zones.pickup_location_id)\n\n\nEt l’utiliser dans COPY ... TO ... :\n\nrequest <- result |>\n  sql_render(con = con)\n\ndbExecute(conn, glue::glue(\"COPY ({DBI::SQL(request)}) TO 'monfichier.parquet' (FORMAT PARQUET)\"))\n\nCette méthode prend autour de 2Go de mémoire.\nPour être complet, il est possible d’utiliser uniquement arrow :\n\nopen_dataset(taxi_dir) |>\n  filter(year == 2016) |>\n  mutate(wday = wday(pickup_datetime)) |>\n  write_dataset(\"fichier_intermediaire\")\n\nCette méthode consomme 7Go de mémoire.\nLe classement final :\n\n\n\nméthode\nmémoire\ntemps\n\n\n\n\ntbl() |> collect() |> write_parquet()\n50Go\n240s\n\n\ntbl() |> to_arrow() |> write_parquet()\n25Go\n300s\n\n\ntbl() |> to_arrow() |> write_dataset()\n15Go\n305s\n\n\nopen_dataset() |> write_dataset()\n7Go\n325s\n\n\nCOPY ... TO...\n2Go\n145s\n\n\n\nUn exemple sur la conversion d’un cas réel est disponible ici\n\n\n\n\n\n\nNote\n\n\n\nSi vous devez utiliser des fichiers intermédiaires pour des données pérennes, utilisez le format parquet.\nSi vos fichiers sources sont vraiment volumineux, duckdb sera plus efficace que arrow\n\n\n\n\nPour les données à courte durée de vie\nPlusieurs solutions sont possibles. Vous pouvez bien sûr utiliser parquet comme précédemment mais vous pouvez aussi créer une table dans duckdb. copy_to sait copier des données issues d’une requête dbplyr vers la base de données sans récupérer les données dans R :\n\nrequest <- tbl(con, glue(\"{taxi_dir}/**/*.parquet\")) |>\n  filter(year == 2016) |>\n  mutate(wday = wday(pickup_datetime)) |>\n  head()\n\ntable_request <- copy_to(con, request, \"table_request\")\n\nAttention, cette requête copie réellement les données en base, elle peut donc prendre un certain temps.\nVous pouvez maintenant utiliser la variable table_request qui contient en pratique l’équivalent de l’ordre tbl(con, \"table_request\").\n\ntable_request\n\n# Source:   table<table_request> [6 x 25]\n# Database: DuckDB v0.9.1 [unknown@Linux 5.4.0-156-generic:R 4.3.1/tuto_duckdb.db]\n  vendor_name pickup_datetime     dropoff_datetime    passenger_count\n  <chr>       <dttm>              <dttm>                        <dbl>\n1 CMT         2016-04-04 15:52:55 2016-04-04 16:15:38               1\n2 VTS         2016-04-04 15:52:55 2016-04-04 16:07:48               1\n3 CMT         2016-04-04 15:52:56 2016-04-04 16:28:01               1\n4 CMT         2016-04-04 15:52:56 2016-04-04 16:18:50               1\n5 VTS         2016-04-04 15:52:56 2016-04-04 16:02:42               1\n6 VTS         2016-04-04 15:52:56 2016-04-04 15:54:40               6\n# ℹ 21 more variables: trip_distance <dbl>, pickup_longitude <dbl>,\n#   pickup_latitude <dbl>, rate_code <chr>, store_and_fwd <chr>,\n#   dropoff_longitude <dbl>, dropoff_latitude <dbl>, payment_type <chr>,\n#   fare_amount <dbl>, extra <dbl>, mta_tax <dbl>, tip_amount <dbl>,\n#   tolls_amount <dbl>, total_amount <dbl>, improvement_surcharge <dbl>,\n#   congestion_surcharge <dbl>, pickup_location_id <dbl>,\n#   dropoff_location_id <dbl>, month <dbl>, year <dbl>, wday <dbl>\n\n\n\n\n\n\n\n\nle format de données de duckdb n’est pas encore stable\n\n\n\nSi vous stockez des données en base, à la prochaine mise à jour de duckdb, il vous préviendra au lancement qu’il n’arrive pas à lire le fichier.\nSi ça vous arrive, vous devrez réinstaller l’ancienne version de duckdb, exporter vos données et les réimporter avec la nouvelle version.\nPour stocker des données au delà de quelques heures/jours, privilégiez le format parquet."
  },
  {
    "objectID": "introduction-duckdb.html#les-limitations-de-dbplyr",
    "href": "introduction-duckdb.html#les-limitations-de-dbplyr",
    "title": "Traiter des données massives en R avec duckdb",
    "section": "Les limitations de dbplyr",
    "text": "Les limitations de dbplyr\nComme tous les outils, dbplyr a des limitations :\n\ntous les ordres dplyr ne sont pas compris (rowwise, …)\nune compréhension de SQL peut vous aider à corriger des erreurs\nglobalement incompatible avec les fonctions de R base.\n\nEt comme pour arrow, certains comportements différents de dplyr peuvent être perturbants :\n\ncertaines astuces R ne passent pas (sommer des booleens…)\nstringr accepte une regexp comme [:punct:] (qui n’est normalement pas valide), pour duckdb il faudra utiliser [[:punct:]]\n\nIl y en a certainement d’autres."
  },
  {
    "objectID": "introduction-duckdb.html#faut-il-passer-à-sql",
    "href": "introduction-duckdb.html#faut-il-passer-à-sql",
    "title": "Traiter des données massives en R avec duckdb",
    "section": "Faut-il passer à SQL ?",
    "text": "Faut-il passer à SQL ?\nÇa n’est pas une obligation… mais ça peut être pratique.\nduckdb est une base de données, ça veut dire que son langage natif est le SQL et malgré l’expressivité des packages tidyverse, ils ne pourront jamais l’égaler. Par ailleurs, duckdb est orienté traitement statistique, il intègre donc beaucoup de fonctions qui peuvent vous être très utiles.\nMais commençons tout doucement.\n\nDes (petits) bouts de SQL\nQuand dbplyr ne comprend pas une fonction, il la passe directement à la base de données.\nImaginons que nous voulions trouver la plus grande valeur entre les lignes fare_amount et tip_amount. Avec dplyr, nous pourrions utiliser rowwise mais cette fonction n’est pas connue de duckdb ni de arrow. Nous pouvons utiliser la fonction GREATEST de duckdb :\n\ntaxi |>\n  filter(year == 2018, month == 1) |>\n  mutate(greatest = GREATEST(fare_amount, tip_amount)) |>\n  select(fare_amount, tip_amount, greatest) |>\n  head(3)\n\n# Source:   SQL [3 x 3]\n# Database: DuckDB v0.9.1 [unknown@Linux 5.4.0-156-generic:R 4.3.1/tuto_duckdb.db]\n  fare_amount tip_amount greatest\n        <dbl>      <dbl>    <dbl>\n1         7.5       1.65      7.5\n2        28.5       5.86     28.5\n3         8         0         8  \n\n\nduckdb est orienté traitement statistique, si vous avez besoin d’une fonction non implémentée en dplyr, elle existe peut-être dans duckdb et en particulier dans les fonctions d’agrégat\nCe point est vraiment très pratique en particulier si vous n’avez pas envie de chercher quelles fonctions de stringr ou lubridate sont traduites. Il suffit d’aller voir les pages de documentation de duckdb et en particulier (mais les autres sont intéressantes aussi !) :\n\nles fonctions de chaines de caractères\nles fonctions de calcul sur les dates\nles fonctions d’extraction d’élements de dates\nles fonctions de “pattern matching”\n…\n\nPour trouver votre bonheur.\n\n\n\nUn exemple, nous avons une table avec 3 colonnes\n\ntbl(con, \"mon_tb\")\n\n# Source:   table<mon_tb> [2 x 3]\n# Database: DuckDB v0.9.1 [unknown@Linux 5.4.0-156-generic:R 4.3.1/tuto_duckdb.db]\n  adresse                            nom_de_fichier          date      \n  <chr>                              <chr>                   <date>    \n1 1, RUE DE BERCY, 92500 Paris       fichier_1993.csv        2023-09-10\n2 3 rue de bagnolet. 93000 Montreuil nom_de_fichier_2003.csv 2019-03-05\n\n\net nous voulons :\n\nnormaliser la colonne adresse en passant en minuscule et en ne gardant que les chiffres et les lettres\nrécupérer l’année du nom du fichier sous forme d’entier\navoir la date du 1er janvier de l’année récupérée ci-dessus\ntrouver le 1er du mois de la colonne date\n\nVoici la commande finale en utilisant des ordres duckdb:\n\ntbl(con, \"mon_tb\") |>\n  mutate(\n    # [^a-z0-9 ] indique tous les caractères SAUF ceux cités (et ceux entre les deux bornes)\n    # le 'g' final indique que le replacement doit être global\n    adresse = REGEXP_REPLACE(LOWER(adresse), '[^a-z0-9 ]+', '', 'g'),\n    # on peut mixer les fonctions dplyr avec les fonctions de duckdb\n    annee_extraite = as.integer(REGEXP_EXTRACT(nom_de_fichier, '([0-9]+)')),\n    premier_janvier_annee_extraite = MAKE_DATE(annee_extraite, 1L, 1L),\n    premier_du_mois = DATE_TRUNC('month', date)\n  ) |>\n  select(adresse, premier_janvier_annee_extraite, premier_du_mois)\n\n# Source:   SQL [2 x 3]\n# Database: DuckDB v0.9.1 [unknown@Linux 5.4.0-156-generic:R 4.3.1/tuto_duckdb.db]\n  adresse                           premier_janvier_annee_extr…¹ premier_du_mois\n  <chr>                             <date>                       <date>         \n1 1 rue de bercy 92500 paris        1993-01-01                   2023-09-01     \n2 3 rue de bagnolet 93000 montreuil 2003-01-01                   2019-03-01     \n# ℹ abbreviated name: ¹​premier_janvier_annee_extraite\n\n\nIl est quand même sain d’avertir le lecteur que certaines fonctions viennent de duckdb afin qu’il ne cherche pas des heures dans la documentation de stringr et lubridate, l’utilisation des majuscules me parait une piste intéressante.\n\n\nDu SQL spécifique\nduckdb implémente la norme SQL et un peu plus. Voici quelques ordres qui peuvent être intéressants :\n\nSUMMARIZE : calcul un certain nombre d’aggrégat (min, max, moyenne, …) sur une table\nPIVOT et UNPIVOT : similaire à tidyr::pivot_longer et tidyr::pivot_wider\nGROUP BY ALL : permet de ne pas répéter les colonnes du SELECT\nSELECT EXCLUDE : permet d’exclure des colonnes d’un SELECT\nGROUP BY CUBE / GROUP BY GROUPING SETS permet de réaliser plusieurs GROUP BY dans la même requête\n\nPour en savoir plus, la documentation de duckdb.\n\n\nPasser de SQL à dbplyr\nQuand vous manipulez plusieurs jeux de données importants, il peut être plus pratique/lisible de passer par une vue consolidée avec les jointures déjà faites plutôt que de refaire les left_join/right_join/… à chaque fois et de passer ensuite à dplyr.\nEn l’occurrence, nous allons créer une vue consolidée (en SQL, une vue est une table virtuelle qui est calculée en temps réel, aucune donnée n’est stockée), intégrant le nom de la zone de pickup et nous laisserons tout la partie calcul en tidyverse :\n\ndbExecute(con, glue(\"CREATE OR REPLACE VIEW courses AS \nSELECT * FROM\n  read_parquet('{taxi_dir}/**/*.parquet') AS taxi,\n  zones\nWHERE taxi.pickup_location_id = zones.pickup_location_id\"))\n\n[1] 0\n\n\n\n\n\n\n\n\nNote\n\n\n\nSi vous ne savez pas du tout faire de SQL, vous pouvez utiliser la commande dbplyr::sql_render() pour voir ce que fait dbplyr et vous en inspirer :\n\ntbl(con, glue(\"{taxi_dir}/**/*.parquet\")) |>\n  left_join(tbl(con, \"zones\")) |>\n  dbplyr::sql_render()\n\nJoining with `by = join_by(pickup_location_id)`\n\n\n<SQL> SELECT\n  \"/nfs/partage-r-sas/exemples/taxis-data/nyc-taxi/**/*.parquet\".*,\n  pickup_zone\nFROM \"/nfs/partage-r-sas/exemples/taxis-data/nyc-taxi/**/*.parquet\"\nLEFT JOIN zones\n  ON (\"/nfs/partage-r-sas/exemples/taxis-data/nyc-taxi/**/*.parquet\".pickup_location_id = zones.pickup_location_id)\n\n\n\n\nVous pouvez maintenant utilisez la vue courses qui intègre les données des zones y compris les noms :\n\ntbl(con, \"courses\")\n\n# Source:   table<courses> [?? x 26]\n# Database: DuckDB v0.9.1 [unknown@Linux 5.4.0-156-generic:R 4.3.1/tuto_duckdb.db]\n   vendor_name pickup_datetime     dropoff_datetime    passenger_count\n   <chr>       <dttm>              <dttm>                        <dbl>\n 1 CMT         2019-04-01 09:25:13 2019-04-01 09:45:09               2\n 2 CMT         2019-04-01 09:48:12 2019-04-01 10:12:30               1\n 3 VTS         2019-04-01 09:39:09 2019-04-01 10:04:10               6\n 4 VTS         2019-04-01 09:24:43 2019-04-01 09:40:21               3\n 5 CMT         2019-04-01 09:07:50 2019-04-01 09:11:33               1\n 6 CMT         2019-04-01 09:19:22 2019-04-01 09:35:09               1\n 7 CMT         2019-04-01 09:40:28 2019-04-01 09:51:47               1\n 8 CMT         2019-04-01 09:53:09 2019-04-01 09:59:47               1\n 9 VTS         2019-04-01 09:09:56 2019-04-01 09:24:01               1\n10 VTS         2019-04-01 09:27:16 2019-04-01 09:43:27               1\n# ℹ more rows\n# ℹ 22 more variables: trip_distance <dbl>, pickup_longitude <dbl>,\n#   pickup_latitude <dbl>, rate_code <chr>, store_and_fwd <chr>,\n#   dropoff_longitude <dbl>, dropoff_latitude <dbl>, payment_type <chr>,\n#   fare_amount <dbl>, extra <dbl>, mta_tax <dbl>, tip_amount <dbl>,\n#   tolls_amount <dbl>, total_amount <dbl>, improvement_surcharge <dbl>,\n#   congestion_surcharge <dbl>, pickup_location_id <dbl>, …\n\n\n\ntbl(con, \"courses\") |>\n  filter(year == 2016) |>\n  mutate(wday = wday(pickup_datetime)) |>\n  filter(str_detect(pickup_zone, \"irport\")) |>\n  group_by(pickup_zone, wday) |>\n  summarize(mean_trip = mean(trip_distance, na.rm = TRUE)) |>\n  arrange(wday) |>\n  collect() |>\n  pivot_wider(names_from = wday, values_from = mean_trip)\n\n`summarise()` has grouped output by \"pickup_zone\". You can override using the\n`.groups` argument.\n\n\n# A tibble: 3 × 8\n# Groups:   pickup_zone [3]\n  pickup_zone         `1`   `2`   `3`   `4`   `5`   `6`   `7`\n  <chr>             <dbl> <dbl> <dbl> <dbl> <dbl> <dbl> <dbl>\n1 Newark Airport     2.25  2.09  2.24  2.00  1.93  2.44  2.49\n2 LaGuardia Airport  9.87  9.94 10.0  10.1  10.0   9.90  9.82\n3 JFK Airport       18.7  48.9  17.3  15.7  17.3  25.0  15.8 \n\n\n\n\nPasser de dbplyr à SQL\nLa première question est “pourquoi voudrais-je faire ça ?”. Pour utiliser une fonction de duckdb par exemple SUMMARIZE ?\nPasser de dbplyr à SQL est (un peu) plus compliqué que de passer de SQL à dbplyr que nous avons vu juste avant. Il faut obligatoirement utiliser un “stockage” temporaire.\n\nPasser par une table\nUne première solution est d’utiliser une propriété de dplyr::copy_to() qui, quand on lui passe des ordres dplyr, va les copier dans une table de la base de données passée en paramètre :\n\ncourses_janvier_2016 <- tbl(con, glue(\"read_parquet('{taxi_dir}/**/*.parquet')\")) |> \n  filter(year == 2016, month == 1) |>\n  mutate(wday = wday(pickup_datetime)) |>\n  head()\n\n# j'utilise `overwrite = TRUE` dans le cadre de la génération de ce document,\n# vous n'en avez pas forcément besoin\nnouvelle_table <- copy_to(con, courses_janvier_2016, \"courses_janvier_2016\", overwrite = TRUE)\n\nnouvelle_table\n\n# Source:   table<courses_janvier_2016> [6 x 25]\n# Database: DuckDB 0.9.0 [unknown@Linux 6.2.0-36-generic:R 4.3.2/tuto_duckdb.db]\n  vendor_name pickup_datetime     dropoff_datetime    passenger_count\n  <chr>       <dttm>              <dttm>                        <dbl>\n1 CMT         2016-01-01 01:39:49 2016-01-01 02:00:25               2\n2 VTS         2016-01-01 01:39:49 2016-01-01 01:52:50               1\n3 VTS         2016-01-01 01:39:49 2016-01-01 01:48:16               1\n4 VTS         2016-01-01 01:39:49 2016-01-01 01:43:51               1\n5 VTS         2016-01-01 01:39:49 2016-01-01 01:46:31               1\n6 VTS         2016-01-01 01:39:49 2016-01-01 01:46:15               1\n# ℹ 21 more variables: trip_distance <dbl>, pickup_longitude <dbl>,\n#   pickup_latitude <dbl>, rate_code <chr>, store_and_fwd <chr>,\n#   dropoff_longitude <dbl>, dropoff_latitude <dbl>, payment_type <chr>,\n#   fare_amount <dbl>, extra <dbl>, mta_tax <dbl>, tip_amount <dbl>,\n#   tolls_amount <dbl>, total_amount <dbl>, improvement_surcharge <dbl>,\n#   congestion_surcharge <dbl>, pickup_location_id <dbl>,\n#   dropoff_location_id <dbl>, month <dbl>, year <dbl>, wday <dbl>\n\n\nVous pouvez maintenant utiliser des functions dplyr sur la variable nouvelle_table :\nAttention, cette méthode copie réellement les données dans une table temporaire et va donc prendre du temps et de la place.\n\n\nPasser par une vue\nUne deuxième solution est de créer une vue SQL qui ne copiera aucune donnée, pour cela nous allons capturer le SQL générer par dbplyr :\n\nsql <- courses_janvier_2016 |> \n  dbplyr::sql_render(con = con) |>\n  DBI::SQL()\nsql\n\n<SQL> SELECT *, EXTRACT('dow' FROM CAST(pickup_datetime AS DATE) + 0) + 1 AS wday\nFROM read_parquet('/home/nc/travail/R/Rexploration/rdata/nyc-taxi//**/*.parquet')\nWHERE (\"year\" = 2016.0) AND (\"month\" = 1.0)\nLIMIT 6\n\n\net l’utiliser pour créer une vue avec duckdb :\n\ndbExecute(con, glue(\"CREATE OR REPLACE VIEW courses_janvier_2016 AS ({sql})\"))\n\n[1] 0\n\n\nVous pouvez maintenant utiliser la vue courses_janvier_2016 :\n\ntbl(con, \"courses_janvier_2016\")\n\n# Source:   table<courses_janvier_2016> [6 x 25]\n# Database: DuckDB v0.9.1 [unknown@Linux 5.4.0-156-generic:R 4.3.1/tuto_duckdb.db]\n  vendor_name pickup_datetime     dropoff_datetime    passenger_count\n  <chr>       <dttm>              <dttm>                        <dbl>\n1 CMT         2016-01-01 01:39:49 2016-01-01 02:00:25               2\n2 VTS         2016-01-01 01:39:49 2016-01-01 01:52:50               1\n3 VTS         2016-01-01 01:39:49 2016-01-01 01:48:16               1\n4 VTS         2016-01-01 01:39:49 2016-01-01 01:43:51               1\n5 VTS         2016-01-01 01:39:49 2016-01-01 01:46:31               1\n6 VTS         2016-01-01 01:39:49 2016-01-01 01:46:15               1\n# ℹ 21 more variables: trip_distance <dbl>, pickup_longitude <dbl>,\n#   pickup_latitude <dbl>, rate_code <chr>, store_and_fwd <chr>,\n#   dropoff_longitude <dbl>, dropoff_latitude <dbl>, payment_type <chr>,\n#   fare_amount <dbl>, extra <dbl>, mta_tax <dbl>, tip_amount <dbl>,\n#   tolls_amount <dbl>, total_amount <dbl>, improvement_surcharge <dbl>,\n#   congestion_surcharge <dbl>, pickup_location_id <dbl>,\n#   dropoff_location_id <dbl>, month <dbl>, year <dbl>, wday <dbl>\n\n\n\n\nAvantage/inconvénient\nLa première méthode devra générer les données, ce qui prendra du temps et de l’espace disque, mais les calculs ultérieurs seront plus rapides, la deuxième ne génère aucune copie mais les calculs ultérieurs seront plus lents. La meilleure solution dépendra de votre cas.\n\n\n\nSQL est compliqué !\nDans cette partie, nous allons voir exemple de conversion d’un calcul réel de data.table à SQL en passant par dplyr. Cette exemple est tiré du même cas réel que celui traité ici.\nLa version initiale sépare le traitement en dix sous-traitements et les réunis à la fin :\n\ncalcul_total_trivar <- function(var1,var2,var3){\n  full_data <- lapply(0:9, function(i){\n    data_etude <- read_parquet(paste0(chemin_export_tables,\"data_etude_\",i,\".parquet\"),col_select=c(\"year\",var1,var2,var3)) %>%\n      filter(year %in% c(2012,2019,2021,2022))\n    stat_annee <- data_etude[  , .(total = sum(eval(parse(text=(var1))), na.rm=TRUE)), by=c(\"year\",var2,var3)]\n    rm(\"data_etude\")\n    return(stat_annee)\n  })\n\n  full_data <- rbindlist(full_data)\n  formula <- as.formula(paste0(paste(var2,var3,sep=\"+\"),\"~year\"))\n  total_var1_var2_var3 <- dcast(data=full_data,formula,fun.aggregate=sum)\n  return(total_var1_var2_var3)\n}\n\ntic()\nres <- calcul_total_trivar(\"parc_01_01\", \"co2_reel_tr\", \"crit_air\")\ntoc()\n\n(et après on dit que SQL est compliqué…)\nLa version dbplyr est nettement plus lisible :\n\ndbplyr_calcul_total_trivar <- function(var1, var2, var3) {\n  tbl(con, \"output/etude/**/*.parquet\") %>%\n    filter(year %in% c(2012,2019,2021,2022)) %>%\n    group_by(year, {{var2}}, {{var3}}) %>%\n    summarize(total = sum({{var1}}, na.rm = TRUE)) %>%\n    collect() %>%\n    pivot_wider(names_from = year, values_from = total) %>%\n    arrange({{var2}}, {{var3}})\n}\ntic()\nres <- dbplyr_calcul_total_trivar(parc_01_01, co2_reel_tr, crit_air)\ntoc()\n\nMais duckdb a une fonction pivot qui fait tout le travail pour nous et voici la version finale :\n\ndbuckdb_calcul_total_trivar <- function(var1, var2, var3) {\n  DBI::dbGetQuery(con, glue::glue(\"\n  PIVOT read_parquet('output/etude/**/*.parquet')\n  ON year IN (2012, 2019, 2021, 2022)\n  USING SUM({var1})\n  GROUP BY {var2}, {var3}\n  ORDER BY {var2}, {var3}\"))\n}\ntic()\nres <- duckdb_calcul_total_trivar(\"parc_01_01\", \"co2_reel_tr\", \"crit_air\")\ntoc()\n\nLa version initiale met 140s et utilise 4,5Go de mémoire, les deux versions dplyr et duckdb utilise 400Mo et mettent 10s. Est-ce qu’il y a vraiment un avantage à passer à SQL ? A vous de voir.\n\n\n\n\n\n\nPetit point sur le fonctionnement de duckdb\n\n\n\nduckdb est une base de données à part entière, à savoir que le traitement d’une requête intègre une phase de planification et d’optimisation. duckdb va analyser les différentes façons possible d’exécuter la requête et choisir celle qui lui semble la plus pertinente.\nVous pouvez voir le plan d’exécution en utilisant la commande EXPLAIN :\n\nEXPLAIN SELECT * FROM tbl;\n\nIl est par ailleurs multi-processeurs. Suivant les tâches qu’il a à effectuer, il pourra les scinder et les faire exécuter en parallèle par différent processeurs (d’où le paramètre threads que nous passons en paramètre)."
  },
  {
    "objectID": "introduction-duckdb.html#quelques-trucs-en-vracs",
    "href": "introduction-duckdb.html#quelques-trucs-en-vracs",
    "title": "Traiter des données massives en R avec duckdb",
    "section": "Quelques trucs en vracs",
    "text": "Quelques trucs en vracs\n\nUtilisez toujours la dernière version de R et des packages arrow et duckdb\narrow et duckdb sont encore jeunes et évoluent très rapidement, il est fortement conseillé d’utiliser les dernières versions et sur les serveurs du SDES d’utiliser la dernière version de R.\n\n\nPartitionner vos données\nCe point n’est pas vraiment un “truc” mais plutôt une “base” : si une variable est souvent utilisée dans un filtre, une partition peut vous permettre de réduire vos temps d’au moins un ordre de grandeur voire plus.\nEt évidemment, la commande COPY ... TO ... de duckdb permet de partitionner :\n\ndbExecute(con, glue::glue(\"COPY (SELECT * FROM courses_taxi) \n                          TO 'dataset_par_annee' (FORMAT PARQUET, PARTITION_BY (year, month))\"))\n\n\n\nSimplifier l’utilisation des fichiers parquet\nIl peut être assez lassant de devoir utiliser à chaque fois le nom complet du fichier ou du dataset parquet. Pour éviter ça, vous pouvez passer par une vue :\n\ndbExecute(con, glue::glue(\"CREATE OR REPLACE VIEW courses_taxi AS\n                          SELECT * FROM read_parquet('{taxi_dir}/**/*.parquet')\"))\n\nDans la suite de votre code vous pourrez utiliser directement courses_taxi soit dans un appel à tbl() soit dans une requête :\n\ndbGetQuery(con, \"SELECT * FROM courses_taxi LIMIT 1\")\n\ntbl(con, \"courses_taxi\") |>\n  filter(year == 2016) |>\n  head()\n\n\n\nVous aurez peut-être (encore) à séparer vos traitements en plusieurs blocs\nduckdb sait gérer les limites mémoires par débordement : quand il n’a plus assez de mémoire il écrit sur disque. Evidemment, ce mécanisme de débordement implique des temps de traitement nettement plus long (au moins un ordre de grandeur).\nDans ces cas là, il peut être vraiment intéressant de revenir à un traitement par bloc. Vous pouvez bien sûr le faire en filtrant (dlyr::filter( ...)) sur vos fichiers parquet et idéalement en filtrant sur une partition parquet pour être efficace.\n\n\nredémarrer votre session de temps en temps\nduckdb a tendance à consommer de la mémoire qui n’est jamais libérée. Redémarrer votre session de temps en temps :\nMenu Session => Restart R\n\n\nPourquoi faut-il éviter d’utiliser arrow::open_dataset(...) >| to_duckdb(con) |> ...\nCette méthode est tout à fait valide, d’autant qu’on peut passer une connexion (avec ses limites) à to_duckdb()\nNéanmoins, même si arrow::open_dataset(...) >| to_duckdb() passe uniquement un objet, une partie du traitement sera réalisée par arrow et une partie par duckdb, les deux consommeront de la mémoire et la limite mémoire que vous aurez fixée ne s’appliqueront qu’à duckdb().\n(De la même façon, n’utilisez jamais duckdb::duckdb_register() sur un objet arrow, ça marchera mais ne sera pas optimisé.)\nAvec tbl(), la limite mémoire sera entièrement gérée par duckdb et donc respectée.\nSur des datasets de petite taille vous ne verrez pas d’impact mais sur sur des tailles conséquentes c’est toute la différence entre un traitement qui aboutit et un traitement qui s’arrête au milieu faute de mémoire.\n\n\nPlantage par manque de mémoire\nDans certains cas très particuliers, j’ai constaté que mes traitements sur des fichiers volumineux pouvaient planter pour un problème de mémoire sur un simple COPY ... TO ... . Ceci peut être dû à un fichier bizarrement créé. Je ne détaillerais pas d’où vient le problème mais vous pouvez vérifier en passant la commande suivante :\n\n\n\n\ndbGetQuery(con, glue(\"SELECT row_group_num_rows, (row_group_bytes / 1024 ^ 2) AS row_group_megaoctet\n                     FROM parquet_metadata('{fichier_problematique}') LIMIT 1\"))\n\n  row_group_num_rows row_group_megaoctet\n1            7066766            747.6689\n\n\nSi la colonne row_group_num_rows est bien supérieure à 1 000 000 et la colonne row_group_megaoctet est proche voire supérieure au Go comme ici , le problème peut venir de là. Une solution est de réduire le nombre de threads de duckdb à “1” le temps de passer le traitement :\n\ncon <- dbConnect(\n  duckdb(), \n  dbdir = \"tuto_duckdb.db\", \n  config = list(\n    memory_limit = \"10G\",\n    threads = \"1\")\n  )\n\nSi vous utilisez régulièrement ce fichier, il peut être intéressant de le corriger définitivement, un COPY ... TO ... ou open_dataset() |> write_dataset() suffisent, arrow fixe par défaut la valeur à 32768 et duckdb à 122880.\nPour plus d’information sur la notion de “row group”.\n\n\nFermer votre base de données\nPensez à fermer votre base de données avec la commande :\n\ndbDisconnect(con, shutdown = TRUE)\n\nça vous évitera des messages d’avertissement comme :\nWarning messages:\n1: Database is garbage-collected, use dbDisconnect(con, shutdown=TRUE) or duckdb::duckdb_shutdown(drv) to avoid this. \n2: Connection is garbage-collected, use dbDisconnect() to avoid this."
  },
  {
    "objectID": "introduction-duckdb.html#quelques-derniers-compléments-sur-duckdb",
    "href": "introduction-duckdb.html#quelques-derniers-compléments-sur-duckdb",
    "title": "Traiter des données massives en R avec duckdb",
    "section": "Quelques derniers compléments sur duckdb",
    "text": "Quelques derniers compléments sur duckdb\nAu delà du CSV et de parquet, duckdb sait lire le format json, sait gérer les données imbriquées et les structures complexes.\nDes extensions sont disponibles pour traiter des données SIG, interroger une base de données postgres/mysql/sqlite ou encore interroger des fichiers parquet à travers internet.\nEt duckdb est encore jeune, chaque version apporte son lot d’évolution. La 0.9.0 a nettement amélioré la gestion de la mémoire."
  },
  {
    "objectID": "introduction-duckdb.html#conclusion",
    "href": "introduction-duckdb.html#conclusion",
    "title": "Traiter des données massives en R avec duckdb",
    "section": "Conclusion",
    "text": "Conclusion\nQue ça soit avec dplyr ou nativement, comme arrow, duckdb rend rapide des calculs qui étaient lents et possible des calculs qui ne l’étaient pas.\nContrairement à arrow, il est capable de faire des jointures entre des tables de plusieurs dizaines voire centaines de millions de lignes et de donner les résultats en quelques secondes ou minutes.\nLe format de données interne est très performant (encore plus rapide que parquet) mais n’est pas encore stabilisé, il est donc déconseillé de l’utiliser pour des données pérennes.\n\nduckdb pour quoi faire ?\nduckdb peut intervenir à plusieurs phases de vos travaux :\n\npour nettoyer vos données\npour regarder ce qu’il y a dedans\npour faire vos études\npour faire des chaines de production\n\nSi vous avez du CSV mal formé ou dans un encodage différent de UTF8, vous devrez peut-être passer par d’autres outils pour le rendre lisible.\nduckdb n’est pas adapté :\n\nsi vous ne souhaitez pas traiter de la donnée (au sens stat du terme)\nsi vous voulez faire des transactions\nsi vos données sont mises à jour en temps réel\n\n\n\nComment choisir entre arrow et duckdb\nSi vous avez à choisir entre arrow et duckdb :\n\nsi vous manipulez un fichier unique : arrow et/ou duckdb\narrow n’implémente pas les fonctions dont vous avez besoin : to_duckdb()\nvous avez des jointures conséquentes : duckdb (associé à dbplyr ou pas)\nvous aimez SQL : duckdb\n\nSi je devais donner mon avis personnel (que personne ne demande), je trouve duckdb plus puissant, plus complet et plus souple que arrow et j’en arrive (presque) à apprécier SQL. Le fait de pouvoir utiliser toutes les fonctions de duckdb à travers dplyr donne accès à une boite à outil très intéressante.\n\n\nConvertir un traitement vers duckdb et dbplyr (ou pas)\nJe reprends les mêmes conseils que dans la session arrow, quand vous convertissez un traitement de {dplyr}, faites des tests :\n\nsur quelques lignes et comparer les deux sorties.\nsur les agrégats entre les deux méthodes (la fonction SUMMARIZE peut vous aider)\nsur le fonctionnement de vos expressions rationnelles\nsur la conversion des dates par lubridate (calcul de max sur les dates, que donne un NA…)\n…"
  },
  {
    "objectID": "introduction-duckdb.html#quelques-références",
    "href": "introduction-duckdb.html#quelques-références",
    "title": "Traiter des données massives en R avec duckdb",
    "section": "Quelques références",
    "text": "Quelques références\n\nla documentation de l’API R de duckdb\nla documentation de dbplyr\nla documentation du SQL de duckdb\nManipuler des données volumineuses avec Arrow & DuckDB\nles pages sur parquet et sur l’accès aux bases de données d’utilitR"
  },
  {
    "objectID": "cas-pratique-traitement-duckdb.html",
    "href": "cas-pratique-traitement-duckdb.html",
    "title": "Cas pratique d’une migration d’un traitement vers duckdb",
    "section": "",
    "text": "Ce cas pratique vient du transport. Il sert à préparer des fichiers d’étude à partir de deux jeux de données :\n\nvehicule : les propriétés d’un véhicule (immat, cylindrée, énergie, date de mise en circulation…) fait 70 millions de lignes\nannee : les évènements liés à un véhicule par année (km parcouru, présence dans le parc… ) fait 490 millions de lignes\n\nEt d’une troisième table de 50 lignes avec la nomenclature des carburants.\nDans le code d’origine, afin de tenir sur le serveur, les données vehicule et annee d’origine sont partitionnées à la main en 10 fichiers véhicules et 10 fichiers années correspondants pour générer 10 fichiers d’étude.\nLe traitement actuel est le suivant :\n\ncharge deux fichiers de données parquet en mémoire avec `arrow::read_parquet()\nfait une jointure entre les deux fichiers de données avec dplyr\najoute des colonnes calculées toujours avec dplyr (année de mise en circulation à partir de la date, la quantité de CO2 émise par tranche…)\nécrit le fichier résultat\n\nNotre cas pratique reprend uniquement la génération d’un fichier d’étude sur les 10.\nDans le suite, nous allons rapidement présenter le code d’origine et voir les différentes évolutions possibles."
  },
  {
    "objectID": "cas-pratique-traitement-duckdb.html#le-code-dorigine",
    "href": "cas-pratique-traitement-duckdb.html#le-code-dorigine",
    "title": "Cas pratique d’une migration d’un traitement vers duckdb",
    "section": "Le code d’origine",
    "text": "Le code d’origine\nLe programme d’origine utilise arrow uniquement pour la lecture des données au format parquet. Les calculs sont réalisés avec dplyr.\n\nlibrary(\"arrow\")\nlibrary(\"dplyr\")\nlibrary(\"readxl\")\nlibrary(\"writexl\")\nlibrary(tictoc)\nlibrary(\"data.table\")\n\n\nchemin_donnees_sources <- \"/home/nc/projet/Parc/\"\n\ngrille_carbu <- subset(\n  read_xlsx(\"/home/nc/projet/Nomenclature/Vehicule/nomenclature_carburant_mai_2022.xlsx\"),\n  select=c(carbu_simpl,carbu_det,energ))\n\n### Boucle pour préparer et exporter toutes les tables data_etude_0 à data_etude_9. ###\n\ndata <- left_join(read_parquet(paste0(chemin_donnees_sources,\"data_annee_VP_0.parquet\"),\n                               col_select = c(\"id\",\"year\",\"parc_01_01\",\"parcm\",\"km\")),\n                  read_parquet(paste0(chemin_donnees_sources,\"data_vehicule_VP_0.parquet\"),\n                               col_select = c(\"id\",\"date_mise_en_cir\",\"energ\",\"co2_theorique\",\n                                              \"puis_kw\",\"poids_vide\",\"crit_air\",\"conso_reelle\",\n                                              \"co2_reel\"))\n                  ,by=\"id\") %>%\n  filter(year>=2012) %>%\n  mutate(\n    poids_vide_tr = cut(poids_vide,\n                        breaks=c(min(poids_vide,na.rm=TRUE),1000,1200,1500,1800,2000,\n                                 max(poids_vide,na.rm=TRUE)),\n                        include.lowest=TRUE,\n                        labels=c(\"1 - moins de 1000 kg\",\n                                 \"2 - plus de 1 à 1,2 t\",\n                                 \"3 - plus de 1,2 à 1,5 t\",\n                                 \"4 - plus de 1,5 à 1,8 t\",\n                                 \"5 - plus de 1,8 à 2 t\",\n                                 \"6 - plus de 2 t\")),\n    annee_mise_en_cir = as.numeric(substr(date_mise_en_cir,1,4)),\n    age = as.numeric(difftime(as.IDate(paste0(year,\"-01-01\")),date_mise_en_cir,units=\"days\")/365.25),\n    age_tr = cut(age,\n                 breaks=c(min(age,na.rm=TRUE),2,5,10,15,20,25,max(age,na.rm=TRUE)),\n                 include.lowest=TRUE,\n                 labels=c(\"1 - moins de 2 ans\",\n                          \"2 - plus de 2 à 5 ans\",\n                          \"3 - plus de 5 à 10 ans\",\n                          \"4 - plus de 10 à 15 ans\",\n                          \"5 - plus de 15 à 20 ans\",\n                          \"6 - plus de 20 à 25 ans\",\n                          \"7 - plus de 25 ans\")),\n    co2_reel = as.numeric(co2_reel),\n    co2_reel_tr = cut(co2_reel,\n                      breaks=c(min(co2_reel,na.rm=TRUE),125,150,175,200,max(co2_reel,na.rm=TRUE)),\n                      include.lowest=TRUE,\n                      labels=c(\"1 - moins de 125 g/km\",\n                               \"2 - plus de 125 à 150 g/km\",\n                               \"3 - plus de 150 à 175 g/km\",\n                               \"4 - plus de 175 à 200 g/km\",\n                               \"5 - plus de 200 g/km\")),\n    co2_reel_tr = ifelse(crit_air==\"E\",\"0 - véhicule électrique\",co2_reel_tr),\n    co2_reel_tr = ifelse(is.na(co2_reel_tr),\"NA - inconnu\",co2_reel_tr),\n    co2_theorique = as.numeric(co2_theorique),\n    co2_theorique_tr = cut(co2_theorique,\n                           breaks=c(min(co2_theorique,na.rm=TRUE),125,150,175,200,\n                                    max(co2_theorique,na.rm=TRUE)),\n                           include.lowest=TRUE,\n                           labels=c(\"1 - moins de 125 g/km\",\n                                    \"2 - plus de 125 à 150 g/km\",\n                                    \"3 - plus de 150 à 175 g/km\",\n                                    \"4 - plus de 175 à 200 g/km\",\n                                    \"5 - plus de 200 g/km\")),\n    co2_theorique_tr = ifelse(crit_air==\"E\",\"0 - véhicule électrique\",co2_theorique_tr),\n    co2_theorique_tr = ifelse(is.na(co2_theorique_tr),\"NA - inconnu\",co2_theorique_tr),\n    puis_kw_tr = cut(puis_kw,\n                     breaks=c(min(puis_kw,na.rm=TRUE),55,70,85,100,max(puis_kw,na.rm=TRUE)),\n                     include.lowest=TRUE,\n                     labels=c(\"1 - moins de 55 kw\",\n                              \"2 - plus de 55 à 70 kw\",\n                              \"3 - plus de 70 à 85 kw\",\n                              \"4 - plus de 85 à 100 kw\",\n                              \"5 - plus de 100 kw\")),\n    puis_kw_tr = ifelse(is.na(puis_kw_tr),\"NA - inconnu\",puis_kw_tr)\n  ) %>%\n\n  left_join(grille_carbu,by=\"energ\") %>%\n\n  mutate(\n    carbu_agreg = ifelse(carbu_det %in% c(\"Diesel\",\"Biodiesel\",\"Diesel HNR\",\"Diesel HR\"),\n                         \"Diesel (y compris hybrides)\",\n                         \"Essence et autres énergies\"),\n    carbu_agreg2 = ifelse(carbu_simpl %in% c(\"Diesel\",\"Essence\"),\n                          \"Diesel et Essence thermiques\",\n                          \"Autres énergies\")\n  )\n\nwrite_parquet(data, \"origine.parquet\")\n\n\n\n\n\n\n\nNote\n\n\n\nLe code fonctionne parfaitement et le temps de traitement est de l’ordre de 15min pour une consommation en pic 25Go de mémoire.\n\n\nIl pourrait certainement être optimisé, le mutate réalisé à la fin pourrait l’être directement sur la table grille_carbu au début pour éviter de devoir modifier les millions de lignes mais ça restera de l’optimisation à la marge (test fait, le traitement passe à 12min).\nLes principaux problèmes de ce code, (écrit à une époque ou arrow n’était pas encore vraiment utilisable sur nos serveurs) sont que :\n\nles données sont entièrement chargées en mémoire par arrow::read_parquet()\nles calculs sont réalisés par dplyr qui n’est pas connu pour être rapide\n\nSi ça ne pose pas de problème sur quelques milliers voire centaines de milliers de lignes, ça devient rédhibitoire quand on croise des dizaines de millions de lignes."
  },
  {
    "objectID": "cas-pratique-traitement-duckdb.html#première-étape-utilisation-de-duckdb-et-dplyr",
    "href": "cas-pratique-traitement-duckdb.html#première-étape-utilisation-de-duckdb-et-dplyr",
    "title": "Cas pratique d’une migration d’un traitement vers duckdb",
    "section": "Première étape : utilisation de duckdb et dplyr",
    "text": "Première étape : utilisation de duckdb et dplyr\nDans cette première étape, nous allons essayer d’éviter de charger les données dans R et, surtout, de faire la jointure en mémoire. Pour cela nous allons lire les données avec duckdb et les traiter avec dplyr qui, en sous-main, va utiliser dbplyr (la version de dplyr pour les bases de données).\n\nL’initialisation\nNous allons commencer par charger les packages nécessaires et on créé une variable avec les chemins :\n\nlibrary(arrow)\nlibrary(dplyr)\nlibrary(readxl)\nlibrary(duckdb)\nlibrary(tidyverse)\nlibrary(lubridate)\nlibrary(tictoc)\n\nchemin_donnees_sources <- \"/home/nc/projet/Parc/Parc_2022/Donnees_individuelles/\"\n\nPuis nous créons une base de données dans le fichier test.duckdb, on limite la mémoire à 10Go et le nombre de CPU utilisé à 4 pour ne pas écrouler le serveur.\n\ncon <- DBI::dbConnect(duckdb(), dbdir = \"test.duckdb\",            \n                      config=list(\"memory_limit\"=\"10GB\",\n                                  \"threads=4\"))\n\n\n\nCréation des tables de base de données à utiliser\nLe référentiel des carburants n’étant pas un fichier parquet, on écrit les données dans la base avec la commande DBI::dbWriteTable()\nOn créé les objets vehicule, annee et grille_carbu avec dplyr::tbl(), cette fonction transforme une table de base de données ou un fichier parquet en objet utilisable par dplyr.\n\nread_xlsx(\"/home/nc/projet/Nomenclature/Vehicule/nomenclature_carburant_mai_2022.xlsx\") |>\n  select(carbu_simpl,carbu_det,energ) |>\n  dbWriteTable(con, \"grille_carbu\", value = _, overwrite = TRUE)\ngrille_carbu <- tbl(con, \"grille_carbu\")\n\nvehicule <- tbl(con, file.path(chemin_donnees_sources, \"data_vehicule_VP_0.parquet\")) %>%\n  select(id, date_mise_en_cir,energ, co2_theorique,puis_kw,poids_vide,crit_air,conso_reelle,co2_reel)\n\nannee <- tbl(con, file.path(chemin_donnees_sources, \"data_annee_VP_0.parquet\")) %>%\n  select(id, year, parc_01_01, parcm, km)\n\n\n\nCréation de la requête dbplyr\nJ’ai pris le parti de modifier le code d’origine à minima pour qu’il fonctionne avec dbplyr, les modifications sont à trois niveaux :\n\nchanger les bornes des cut pour les passer à 0 et Inf plutôt que de calculer les min et max\nutiliser lubridate::year() plutôt que as.numeric(substr(date_mise_en_cir,1,4)) (ligne 5)\nutiliser lubridate::as_date() pour calculer l’age du véhicule (ligne 6)\n\n\nrequest <- vehicule %>%\n  right_join(annee, by = c(\"id\")) %>%\n  filter(year >= 2012) %>%\n  mutate(\n    annee_mise_en_cir = year(date_mise_en_cir),\n    age = (as_date(paste0(year, \"-01-01\")) - date_mise_en_cir ) / 365.25,\n    poids_vide_tr = cut(poids_vide,\n                      breaks=c(0,1000,1200,1500,1800,2000,Inf),\n                      include.lowest=TRUE,\n                      labels=c(\"1 - moins de 1000 kg\",\n                               \"2 - plus de 1 à 1,2 t\",\n                               \"3 - plus de 1,2 à 1,5 t\",\n                               \"4 - plus de 1,5 à 1,8 t\",\n                               \"5 - plus de 1,8 à 2 t\",\n                               \"6 - plus de 2 t\")),\n    age_tr = cut(age,\n                 breaks=c(0,2,5,10,15,20,25,Inf),\n                 include.lowest=TRUE,\n                 labels=c(\"1 - moins de 2 ans\",\n                          \"2 - plus de 2 à 5 ans\",\n                          \"3 - plus de 5 à 10 ans\",\n                          \"4 - plus de 10 à 15 ans\",\n                          \"5 - plus de 15 à 20 ans\",\n                          \"6 - plus de 20 à 25 ans\",\n                          \"7 - plus de 25 ans\")),\n    co2_reel_tr = cut(co2_reel,\n                      breaks=c(0,100,130,140,155,175,200,Inf),\n                      include.lowest=TRUE,\n                      labels=c(\"1 - moins de 123 g/km\",\n                               \"2 - plus de 123 à 138 g/km\",\n                               \"3 - plus de 138 à 165 g/km\",\n                               \"4 - plus de 165 à 190 g/km\",\n                               \"5 - plus de 190 à 213 g/km\",\n                               \"6 - plus de 213 à 226 g/km\",\n                               \"7 - plus de 226 g/km\")),\n    co2_reel_tr = ifelse(crit_air==\"E\",\"0 - véhicule électrique\",co2_reel_tr),\n    co2_reel_tr = ifelse(is.na(co2_reel_tr),\"NA - inconnu\",co2_reel_tr),\n    co2_theorique_tr = cut(co2_theorique,\n                           breaks=c(0,123,138,165,190,213,226,Inf),\n                           include.lowest=TRUE,\n                           labels=c(\"1 - moins de 123 g/km\",\n                                    \"2 - plus de 123 à 138 g/km\",\n                                    \"3 - plus de 138 à 165 g/km\",\n                                    \"4 - plus de 165 à 190 g/km\",\n                                    \"5 - plus de 190 à 213 g/km\",\n                                    \"6 - plus de 213 à 226 g/km\",\n                                    \"7 - plus de 226 g/km\")),\n    co2_theorique_tr = ifelse(crit_air==\"E\",\"0 - véhicule électrique\",co2_theorique_tr),\n    co2_theorique_tr = ifelse(is.na(co2_theorique_tr),\"NA - inconnu\",co2_theorique_tr),\n    puis_kw_tr = cut(puis_kw,\n                     breaks=c(0,55,70,85,100,Inf),\n                     include.lowest=TRUE,\n                     labels=c(\"1 - moins de 55 kw\",\n                              \"2 - plus de 55 à 70 kw\",\n                              \"3 - plus de 70 à 85 kw\",\n                              \"4 - plus de 85 à 100 kw\",\n                              \"5 - plus de 100 kw\"))\n    ) %>%\n  left_join(grille_carbu, by = \"energ\") %>%\n  mutate(\n    carbu_agreg = ifelse(carbu_det %in% c(\"Diesel\",\"Biodiesel\",\"Diesel HNR\",\"Diesel HR\"),\n                         \"Diesel (y compris hybrides)\",\n                         \"Essence et autres énergies\"),\n    carbu_agreg2 = ifelse(carbu_simpl %in% c(\"Diesel\",\"Essence\"),\n                          \"Diesel et Essence thermiques\",\n                          \"Autres énergies\")\n  )\n\nCette commande retourne (quasiment) immédiatement car à ce stade, l’objet request n’est pas encore évalué, on parle de “lazy evaluation”, il contient juste les ordres à passer.\n\n\non exécute la requête\nLa requête est effectivement exécuté au moment de la commande write_parquet(), dbplyr va alors transformer tous les ordres en SQL et les envoyer à la base de données.\n\nrequest %>%\n  to_arrow() %>%\n  write_parquet(\"dbplyr.parquet\")\n\n\n\n\n\n\n\nNote\n\n\n\nCe code prend autour de 150 secondes et consomme au pic 15/16Go de mémoire.\n\n\n\n\n\n\n\n\nAstuce\n\n\n\nLa limite à 10Go de duckdb ne fonctionne visiblement pas, pourquoi ?\nUne partie de la mémoire est utilisée par arrow au moment de la matérialisation et, évidemment, les limites fixées à duckdb ne peuvent pas s’appliquer sur arrow."
  },
  {
    "objectID": "cas-pratique-traitement-duckdb.html#dernière-étape-doptimisation",
    "href": "cas-pratique-traitement-duckdb.html#dernière-étape-doptimisation",
    "title": "Cas pratique d’une migration d’un traitement vers duckdb",
    "section": "Dernière étape d’optimisation",
    "text": "Dernière étape d’optimisation\nPeut-on aller plus loin ? Oui mais pour ça il faut éviter de matérialiser les données.\nIl y a deux solutions :\n\nLa solution arrow::write_dataset()\nLa première solution est d’utiliser arrow::write_dataset() à la place de arrow::write_parquet().\nEn effet, contrairement à write_parquet(), write_dataset() sait lire les données envoyées par la requête dbplyr au fur et à mesure tandis que write_parquet a besoin de la totalité des données pour les écrire.\nChanger les dernières lignes write_dataset() a un impact majeur :\n\nrequest %>%\n  to_arrow() %>%\n  write_dataset(\"dbplyr_parquet\")\n\n\n\n\n\n\n\nNote\n\n\n\nCe code prend autour de 100 secondes et consomme au pic 2Go de mémoire.\n\n\n\n\nLa solution “full” duckdb\nLa seconde solution est d’utiliser la commande native de duckdb pour générer les fichiers parquet : COPY ... TO ...\nSi vous êtes expert en SQL vous pouvez convertir mais ça n’est pas mon cas alors nous allons utiliser la fonction dbplyr::sql_render() et utiliser directement le SQL généré par dbplyr :\n\nsql <- request %>% \n  dbplyr::sql_render(con = con)\n\nLa variable sql contient une requête :\n\n<SQL>\nSELECT\n  *,\n  CASE WHEN (carbu_det IN ('Diesel', 'Biodiesel', 'Diesel HNR', 'Diesel HR')) THEN 'Diesel (y compris hybrides)' WHEN NOT (carbu_det IN ('Diesel', 'Biodiesel', 'Diesel HNR', 'Diesel HR')) THEN 'Essence et autres énergies' END AS carbu_agreg,\n  CASE WHEN (carbu_simpl IN ('Diesel', 'Essence')) THEN 'Diesel et Essence thermiques' WHEN NOT (carbu_simpl IN ('Diesel', 'Essence')) THEN 'Autres énergies' END AS carbu_agreg2\nFROM (\n  SELECT\n    *,\n    CASE\nWHEN (puis_kw <= 55.0) THEN '1 - moins de 55 kw'[...]\n[...]\n\nLe SQL est “brutal” mais :\n\nnous ne sommes pas à un concours de beau code\nl’optimisation c’est le boulot de duckdb\n\nNous allons enfin utiliser le SQL généré pour écrire le fichier :\n\ndbExecute(con, glue::glue(\"COPY ({DBI::SQL(request)}) TO 'test2.parquet'\"))\n\nEt alors ?\n\n\n\n\n\n\nNote\n\n\n\nCe code prend également autour de 100 secondes et consomme 1,7Go de mémoire en pic.\n\n\nOn aurait aussi pu partitionner par exemple sur year si cette variable était beaucoup utilisée pour des filter :\n\ndbExecute(con, glue::glue(\"COPY ({DBI::SQL(request)}) \n                          TO 'dataset_par_annee' (FORMAT PARQUET, PARTITION_BY (year))\"))\n\nOn peut noter également que le fichier parquet généré par duckdb est 20% plus gros que celui de arrow (730Go contre 600).\nNous venons de passer le traitement de 15min et plus de 25Go à 100s et 1,7 ou 2Go. On peut s’arrêter là pour aujourd’hui non ?"
  },
  {
    "objectID": "cas-pratique-traitement-duckdb.html#discussion-sur-la-conversion-de-dplyr-à-dbplyr",
    "href": "cas-pratique-traitement-duckdb.html#discussion-sur-la-conversion-de-dplyr-à-dbplyr",
    "title": "Cas pratique d’une migration d’un traitement vers duckdb",
    "section": "Discussion sur la conversion de dplyr à dbplyr",
    "text": "Discussion sur la conversion de dplyr à dbplyr\nLa conversion de ce traitement en duckdb a demandé un peu de travail essentiellement sur la partie manipulation des dates pour trouver les fonctions lubridate connues de dbplyr.\n\n\n\nDerrière la scène j’ai créé une table table avec les deux colonnes nécessaires que j’ai enregistrée dans duckdb dans la table dates :\n\ndates\n\n# A tibble: 2 × 2\n  date_mise_en_cir  year\n  <date>           <int>\n1 2018-04-02        2022\n2 2020-11-29        2023\n\n\nLe passage :\n\ndates |>\n  mutate(\n    annee_mise_en_cir = as.numeric(substr(date_mise_en_cir, 1, 4)),\n    age = as.numeric(difftime(as.IDate(paste0(year,\"-01-01\")), date_mise_en_cir, units=\"days\")/365.25)\n  )\n\n# A tibble: 2 × 4\n  date_mise_en_cir  year annee_mise_en_cir   age\n  <date>           <int>             <dbl> <dbl>\n1 2018-04-02        2022              2018  3.75\n2 2020-11-29        2023              2020  2.09\n\n\nest traduit en :\n\ntbl(con, \"dates\") |>\n  mutate(annee_mise_en_cir = year(date_mise_en_cir)) |>\n  mutate(age = (as_date(paste0(year, \"-01-01\")) - date_mise_en_cir ) / 365.25)\n\n# Source:   SQL [2 x 4]\n# Database: DuckDB 0.9.0 [unknown@Linux 6.2.0-36-generic:R 4.3.2/:memory:]\n  date_mise_en_cir  year annee_mise_en_cir   age\n  <date>           <int>             <dbl> <dbl>\n1 2018-04-02        2022              2018  3.75\n2 2020-11-29        2023              2020  2.09\n\n\nMais dbplyr a une particularité intéressante, quand il ne comprends pas un ordre, il l’envoie tel quel à la base de données. Nous aurions pu nous appuyer sur ce mécanisme pour utiliser la fonction make_date(year, month, day) de duckdb :\n\ntbl(con, \"dates\") |>\n  mutate(annee_mise_en_cir = year(date_mise_en_cir)) |>\n  mutate(age = (make_date(year, 1L, 1L) - date_mise_en_cir ) / 365.25)\n\n# Source:   SQL [2 x 4]\n# Database: DuckDB 0.9.0 [unknown@Linux 6.2.0-36-generic:R 4.3.2/:memory:]\n  date_mise_en_cir  year annee_mise_en_cir   age\n  <date>           <int>             <dbl> <dbl>\n1 2018-04-02        2022              2018  3.75\n2 2020-11-29        2023              2020  2.09\n\n\nVersion nettement plus facile à car faisant uniquement ce qu’il faut sans substr, paste et cie."
  },
  {
    "objectID": "cas-pratique-traitement-duckdb.html#peut-on-utiliser-uniquement-arrow-sur-ce-traitement",
    "href": "cas-pratique-traitement-duckdb.html#peut-on-utiliser-uniquement-arrow-sur-ce-traitement",
    "title": "Cas pratique d’une migration d’un traitement vers duckdb",
    "section": "Peut-on utiliser uniquement arrow sur ce traitement ?",
    "text": "Peut-on utiliser uniquement arrow sur ce traitement ?\nBien sûr, comme pour le point précédent, il y a un travail sur la conversion car arrow :\n\nne gère pas les calculs sur les dates comme dbplyr\nne connait pas base::cut() qui est massivement employé.\n\nSur le premier point, une seule ligne à changer par rapport à notre code dbplyr :\n\ndates %>%\n  mutate(\n    annee_mise_en_cir = year(date_mise_en_cir),\n    age = (as_date(paste0(year, \"-01-01\")) - date_mise_en_cir ) / 365.25\n  )\n\n# A tibble: 2 × 4\n  date_mise_en_cir  year annee_mise_en_cir age          \n  <date>           <int>             <dbl> <drtn>       \n1 2018-04-02        2022              2018 3.750856 days\n2 2020-11-29        2023              2020 2.088980 days\n\n\nArrow n’autorise pas à faire un calcul sur une différence entre deux dates, il faut donc les convertir en jours (as.integer(date) va donner un nombre de jours depuis le 1er janvier 1970) et ensuite on peut travailler dessus comme sur des entiers :\n\ndates |> as_arrow_table() |>\n  mutate(\n    annee_mise_en_cir = year(date_mise_en_cir),\n    age = (as.integer(as_date(paste0(year, \"-01-01\"))) - as.integer(date_mise_en_cir)) / 365.25\n  ) |>\n  collect()\n\n# A tibble: 2 × 4\n  date_mise_en_cir  year annee_mise_en_cir   age\n  <date>           <int>             <int> <dbl>\n1 2018-04-02        2022              2018  3.75\n2 2020-11-29        2023              2020  2.09\n\n\n\n\n\nSur le second point, on peut remplacer cut par un case_when, soit :\n\npoids |>\n  mutate(\n    poids_vide_tr = cut(poids_vide,\n                      breaks=c(0,1000,1200,1500,1800,2000,Inf),\n                      include.lowest=TRUE,\n                      labels=c(\"1 - moins de 1000 kg\",\n                               \"2 - plus de 1 à 1,2 t\",\n                               \"3 - plus de 1,2 à 1,5 t\",\n                               \"4 - plus de 1,5 à 1,8 t\",\n                               \"5 - plus de 1,8 à 2 t\",\n                               \"6 - plus de 2 t\")))\n\n# A tibble: 4 × 2\n  poids_vide poids_vide_tr          \n       <dbl> <fct>                  \n1         10 1 - moins de 1000 kg   \n2       3000 6 - plus de 2 t        \n3       1700 4 - plus de 1,5 à 1,8 t\n4       1300 3 - plus de 1,2 à 1,5 t\n\n\nPar :\n\npoids |>\n  as_arrow_table() |>\n  mutate(\n    poids_vide_tr = case_when(\n      poids_vide <= 1000 ~ \"1 - moins de 1000 kg\",\n      poids_vide <= 1200 ~ \"2 - plus de 1 à 1,2 t\",\n      poids_vide <= 1500 ~ \"3 - plus de 1,2 à 1,5 t\",\n      poids_vide <= 1800 ~ \"4 - plus de 1,5 à 1,8 t\",\n      poids_vide <= 2000 ~ \"5 - plus de 1,8 à 2 t\",\n      poids_vide > 2000 ~ \"6 - plus de 2 t\")) |>\n  collect()\n\n# A tibble: 4 × 2\n  poids_vide poids_vide_tr          \n       <dbl> <chr>                  \n1         10 1 - moins de 1000 kg   \n2       3000 6 - plus de 2 t        \n3       1700 4 - plus de 1,5 à 1,8 t\n4       1300 3 - plus de 1,2 à 1,5 t\n\n\nÀ noter que :\n\nla version arrow::case_when() ne supporte pas l’argument .default de dplyr::case_when()\ndbplyr supporte également cette syntaxe et que la traduction SQL est la même que celle de base::cut())\nje trouve la syntaxe case_when() plus explicite, les limites sont en face du texte correspondant\n\nJe laisse au lecteur le soin de convertir l’ensemble du script.\n\n\n\n\n\n\nNote\n\n\n\nEn utilisant write_dataset() pour écrire les données, le traitement met 80 secondes et utilise 15Go de mémoire."
  },
  {
    "objectID": "cas-pratique-traitement-duckdb.html#conclusions",
    "href": "cas-pratique-traitement-duckdb.html#conclusions",
    "title": "Cas pratique d’une migration d’un traitement vers duckdb",
    "section": "Conclusions",
    "text": "Conclusions\nNous avons réussi à diviser d’un facteur 10 le temps de traitement et la mémoire utilisée avec un investissement en temps raisonnable et surtout en rendant le code beaucoup plus lisible. En passant, il peut être intéressant de passer par des fonctions duckdb.\nIl est même tout à fait envisageable dans le cas présent et avec les deux versions duckdb de traiter l’ensemble des fichiers en une seule passe. Le test montre que, en augmentant la limite de mémoire allouée à duckdb à 15Go, ce qui reste très raisonnable, les deux méthodes permettent de traiter d’un bloc l’ensemble des fichiers en 12min en utilisant autour de 12Go de mémoire.\nLa version “full” arrow met à peu près le même temps mais consomme 150Go de mémoire en pic !\nEt pour finir : le traitement utilisant uniquement duckdb permet de mieux maîtriser la consommation mémoire au prix d’un ralentissement important quand il arrive à la limite fixée. En fixant une limite à 10Go à duckdb pour traiter l’ensemble des fichiers en une seule passe, le temps de traitement passe à 45 min soit 3 fois le temps qu’il faudrait pour traiter les fichiers séparément (duckdb écrit des données temporaires sur disque, ce qui est beaucoup plus lent que de travailler en mémoire). Pour traiter des fichiers particulièrement volumineux, il peut dont être intéressant de continuer à séparer les calculs.\n\n\n\n\n\n\nQuand vos données sont volumineuses\n\n\n\n\nutilisez tbl() ou arrow::open_dataset() plutôt que arrow::read_parquet()\nutilisez duckdb/dbplyr ou arrow plutôt que dplyr\nquand vous avez des jointures sur des grosses tables, utilisez duckdb/dbplyr plutôt que arrow\nutilisez arrow::write_dataset() ou COPY ... TO ... plutôt arrow::write_parquet()\nsi ces outils repoussent (beaucoup) les limites, ils ne sont pas magiques : il peut encore être intéressant de scinder des calculs en plusieurs blocs pour les accélérer voire les rendre possibles."
  },
  {
    "objectID": "presentation_insee.html#les-objectifs",
    "href": "presentation_insee.html#les-objectifs",
    "title": "parquet, arrow et duckdb",
    "section": "Les objectifs",
    "text": "Les objectifs\n\nProposer des solutions pour\n\nmigrer les traitements SAS portant sur des données volumineuses\npermettre aux utilisateurs R d’éviter dans la mesure du possibles les contournements scabreux habituels (traitements par morceaux…)\n\nIntroduire le format parquet\nDiminuer la volumétrie disque utilisée"
  },
  {
    "objectID": "presentation_insee.html#une-stratégie-différenciée",
    "href": "presentation_insee.html#une-stratégie-différenciée",
    "title": "parquet, arrow et duckdb",
    "section": "Une stratégie différenciée",
    "text": "Une stratégie différenciée\n\narrow/duckdb quand il y a besoin\nread_parquet()/write_parquet() + dplyr sinon\n\nOn ne cherche (surtout pas) à vendre arrow/duckdb à tout le monde"
  },
  {
    "objectID": "presentation_insee.html#un-travail-amont-conséquent",
    "href": "presentation_insee.html#un-travail-amont-conséquent",
    "title": "parquet, arrow et duckdb",
    "section": "Un travail amont conséquent",
    "text": "Un travail amont conséquent\n\nprise en main de arrow et duckdb\nétude de cas réels avec “benchmark”"
  },
  {
    "objectID": "presentation_insee.html#du-teasing",
    "href": "presentation_insee.html#du-teasing",
    "title": "parquet, arrow et duckdb",
    "section": "Du teasing",
    "text": "Du teasing\n\nPrésentation des gains constatés sur les cas réels à différents agents “prescripteurs”"
  },
  {
    "objectID": "presentation_insee.html#des-présentations-à-plusieurs-niveaux",
    "href": "presentation_insee.html#des-présentations-à-plusieurs-niveaux",
    "title": "parquet, arrow et duckdb",
    "section": "Des présentations à plusieurs niveaux",
    "text": "Des présentations à plusieurs niveaux\nAu niveau du CODIR SDES élargi :\n\nprésentation de très haut niveau sur parquet, arrow et duckdb\nquelques exemples simples avant/après avec benchmark"
  },
  {
    "objectID": "presentation_insee.html#des-présentations-à-plusieurs-niveaux-1",
    "href": "presentation_insee.html#des-présentations-à-plusieurs-niveaux-1",
    "title": "parquet, arrow et duckdb",
    "section": "Des présentations à plusieurs niveaux",
    "text": "Des présentations à plusieurs niveaux\nAu niveau des sous-directions pour tous les agents intéressés :\n\nun point plus complet et opérationnel sur parquet et les fonctions arrow read_parquet()/write_parquet()\nquelques comparaisons avec les outils actuels\ndes exemples arrow/duckdb plus poussés sur les courses de taxi new yorkais\net sur des exemples réels du SDES"
  },
  {
    "objectID": "presentation_insee.html#un-gt-dédié-aux-utilisateurs-de-données-massives",
    "href": "presentation_insee.html#un-gt-dédié-aux-utilisateurs-de-données-massives",
    "title": "parquet, arrow et duckdb",
    "section": "Un GT dédié aux utilisateurs de données massives",
    "text": "Un GT dédié aux utilisateurs de données massives\nConcerne une 10aine d’agents.\n\nprésentation en profondeur des outils arrow/duckdb\ndes cas pratiques\ntravail sur les sources à convertir (priorités, partition…)"
  },
  {
    "objectID": "presentation_insee.html#un-accompagnement",
    "href": "presentation_insee.html#un-accompagnement",
    "title": "parquet, arrow et duckdb",
    "section": "Un accompagnement",
    "text": "Un accompagnement\nUn accompagnement spécifique au quotidien des utilisateurs de données massives sur :\n\nleur code\nune aide à la conversion de leurs fichiers\n\n(à élargir dans un deuxième temps et au fur et à mesure des capacités)\nEt en parallèle, un site avec de la documentation, la présentation de l’ensemble des cas pratiques, une FAQ…"
  },
  {
    "objectID": "presentation_insee.html#des-retours-très-positifs",
    "href": "presentation_insee.html#des-retours-très-positifs",
    "title": "parquet, arrow et duckdb",
    "section": "Des retours très positifs",
    "text": "Des retours très positifs\n\nà la fois des utilisateurs R mais aussi des selfeurs, séduits par les performances.\nplusieurs selfeurs réticents sur R deviennent demandeurs.\nplusieurs traitements (R) en cours de conversion vers ces nouveaux outils.\n\nCes outils lèvent un point de blocage important de la migration SAS vers R"
  },
  {
    "objectID": "presentation_insee.html#des-difficultés-pour-les-utilisateurs",
    "href": "presentation_insee.html#des-difficultés-pour-les-utilisateurs",
    "title": "parquet, arrow et duckdb",
    "section": "Des difficultés, pour les utilisateurs",
    "text": "Des difficultés, pour les utilisateurs\n\ntout le tidyverse n’est pas implémenté. Il faut parfois contourner avec l’existant\ncertains utilisateurs ont du mal à comprendre l’articulation entre l’existant, arrow, duckdb et dbplyr\narrow/duckdb peuvent planter avec des messages laconiques en cas de manque mémoire"
  },
  {
    "objectID": "presentation_insee.html#des-difficultés-pour-les-utilisateurs-2",
    "href": "presentation_insee.html#des-difficultés-pour-les-utilisateurs-2",
    "title": "parquet, arrow et duckdb",
    "section": "Des difficultés, pour les utilisateurs (2)",
    "text": "Des difficultés, pour les utilisateurs (2)\n“ça marchait avant”, dplyr est plus laxiste que arrow/dbplyr/duckdb :\n\npermet l’addition de booléens\npermet de mixer avec R base\nautorise des expressions rationnelles invalides ([:punct:] à la place de [[:punct:]])\n…"
  },
  {
    "objectID": "presentation_insee.html#des-difficultés-pour-les-utilisateurs-3",
    "href": "presentation_insee.html#des-difficultés-pour-les-utilisateurs-3",
    "title": "parquet, arrow et duckdb",
    "section": "Des difficultés, pour les utilisateurs (3)",
    "text": "Des difficultés, pour les utilisateurs (3)\nUne conversion plus ou moins difficile suivant les packages utilisés :\n\ndplyr “pur”\ndplyr + r-base\ndata.table"
  },
  {
    "objectID": "presentation_insee.html#et-pour-les-admins",
    "href": "presentation_insee.html#et-pour-les-admins",
    "title": "parquet, arrow et duckdb",
    "section": "Et pour les admins",
    "text": "Et pour les admins\n\npar défaut, arrow et duckdb satureront les CPU sur certaines actions :\n\narrow::set_cpu_count(N) (à ajouter dans le Rprofile général ?)\ndirectives threads dans duckdb (doit être ajouté par les utilisateurs)\n\ncertains utilisateurs font des traitements csv -> rds -> parquet… et gardent tous les fichiers intermédiaires."
  },
  {
    "objectID": "presentation_insee.html#conversion-des-fichiers-sas",
    "href": "presentation_insee.html#conversion-des-fichiers-sas",
    "title": "parquet, arrow et duckdb",
    "section": "Conversion des fichiers SAS",
    "text": "Conversion des fichiers SAS\n\nles utilisateurs sont généralement autonomes pour les “petits” fichiers avec les packages R\npour les gros fichiers SAS : readstat est votre ami\n\nNe sous-estimez pas le temps de conversion (1,5 mois pour convertir les 2To de POTE en 0,2Go de fichiers parquet)"
  },
  {
    "objectID": "presentation_insee.html#et-maintenant",
    "href": "presentation_insee.html#et-maintenant",
    "title": "parquet, arrow et duckdb",
    "section": "Et maintenant",
    "text": "Et maintenant\n\nDes débuts encourageants\nEncore beaucoup de travail à prévoir pour :\n\ncontinuer l’accompagnement et la montée en compétence des agents\ncouvrir de nouveaux cas pratiques\nétudier les possibilités de traitement sur les données géographiques, sur les bases postgres…"
  },
  {
    "objectID": "presentation_insee.html#on-charge-quelque-packages-et-variables",
    "href": "presentation_insee.html#on-charge-quelque-packages-et-variables",
    "title": "parquet, arrow et duckdb",
    "section": "On charge quelque packages et variables",
    "text": "On charge quelque packages et variables\n\nlibrary(tidyverse)\nlibrary(arrow)\nlibrary(duckdb)\n\ntaxi_dir <- \"/nfs/partage-r-sas/exemples/taxis-data/nyc-taxi\""
  },
  {
    "objectID": "presentation_insee.html#fixez-des-limites",
    "href": "presentation_insee.html#fixez-des-limites",
    "title": "parquet, arrow et duckdb",
    "section": "Fixez des limites",
    "text": "Fixez des limites\nAvec duckdb, n’utilisez pas de base de données en mémoire et fixer des limites mémoire et CPU :\n\ncon <- DBI::dbConnect(duckdb(), \n                      dbdir = \"ma_base.db\", \n                      config=list(\n                        \"memory_limit\"=\"20GB\", \n                        threads = \"4\")\n                      )\n\nVos traitements passeront sur disques en cas de dépassement du seuil fixé (attention à l’impact sur les performances).\nAvec arrow, fixez le nombre de CPU :\n\narrow::set_cpu_count(4)"
  },
  {
    "objectID": "presentation_insee.html#vous-pouvez-utiliser-duckdb-avec-arrow",
    "href": "presentation_insee.html#vous-pouvez-utiliser-duckdb-avec-arrow",
    "title": "parquet, arrow et duckdb",
    "section": "Vous pouvez utiliser duckdb avec arrow",
    "text": "Vous pouvez utiliser duckdb avec arrow\n\nopen_dataset(taxi_dir) |> filter(year == 2018 & month == 1) |>\n  filter(trip_distance == max(trip_distance, na.rm = TRUE)) |>\n  collect()\n\nError: Filter expression not supported for Arrow Datasets: trip_distance == max(trip_distance, na.rm = TRUE)\nCall collect() first to pull data into R.\n\n\n\nopen_dataset(taxi_dir) |> filter(year == 2018 & month == 1) |>\n  to_duckdb() |>\n  filter(trip_distance == max(trip_distance, na.rm = TRUE)) |>\n  collect()\n\n# A tibble: 1 × 24\n  vendor_name pickup_datetime     dropoff_datetime    passenger_count\n  <chr>       <dttm>              <dttm>                        <dbl>\n1 VTS         2018-01-30 11:41:02 2018-01-30 11:42:09               1\n..."
  },
  {
    "objectID": "presentation_insee.html#evitez-de-mixer-arrow-et-duckdb",
    "href": "presentation_insee.html#evitez-de-mixer-arrow-et-duckdb",
    "title": "parquet, arrow et duckdb",
    "section": "Evitez de mixer arrow et duckdb",
    "text": "Evitez de mixer arrow et duckdb\nUtilisez :\n\ntbl(con, glue::glue(\"read_parquet('{taxi_dir}/**/*.parquet')\"))\n\nPlutôt que :\n\narrow::open_dataset(taxi_dir) |>\n    to_duckdb()\n\nLes deux marcheront, en temps normal, mais le second allouera de la mémoire à arrow ET à duckdb."
  },
  {
    "objectID": "presentation_insee.html#vous-pouvez-passer-de-duckdb-à-dbplyr",
    "href": "presentation_insee.html#vous-pouvez-passer-de-duckdb-à-dbplyr",
    "title": "parquet, arrow et duckdb",
    "section": "Vous pouvez passer de duckdb à dbplyr",
    "text": "Vous pouvez passer de duckdb à dbplyr\nEt ça peut être utile/lisible :\n\ndbExecute(con, glue::glue(\"CREATE OR REPLACE VIEW courses_taxi AS \n                SELECT * FROM '{taxi_dir}/**/*.parquet'\"))\n\ntbl(con, \"courses_taxi\") |>\n  filter(year == 2016, month == 1) |>\n  summarize(trip_distance = mean(trip_distance)) |>\n  collect()"
  },
  {
    "objectID": "presentation_insee.html#et-de-dbplyr-à-duckdb",
    "href": "presentation_insee.html#et-de-dbplyr-à-duckdb",
    "title": "parquet, arrow et duckdb",
    "section": "Et de dbplyr à duckdb",
    "text": "Et de dbplyr à duckdb\nMais il faut passer par un “stockage” intermédiaire.\nSoit une “vraie” table :\n\nrequest <- tbl(con, \"courses_taxi\") |>\n  filter(year == 2016, month == 1) |>\n  summarize(trip_distance = mean(trip_distance))\n\ncopy_to(con, request, \"request\")\n\n\ncopy_to() sait utiliser une requête dbplyr pour peupler une table.\ncopie réellement les données en base"
  },
  {
    "objectID": "presentation_insee.html#et-de-dbplyr-à-duckdb-2",
    "href": "presentation_insee.html#et-de-dbplyr-à-duckdb-2",
    "title": "parquet, arrow et duckdb",
    "section": "Et de dbplyr à duckdb (2)",
    "text": "Et de dbplyr à duckdb (2)\nSoit une vue, en réutilisant le SQL généré ci-dessus :\n\nsql <- request |> dbplyr::sql_render(con = con) |> DBI::SQL()\nsql \n\n<SQL> SELECT AVG(trip_distance) AS trip_distance\nFROM courses_taxi\nWHERE (\"year\" = 2016.0) AND (\"month\" = 1.0)\n\ndbExecute(con, glue::glue(\"CREATE OR REPLACE VIEW ma_vue AS {sql}\"))\n\n[1] 0\n\ntbl(con, \"ma_vue\")\n\n# Source:   table<ma_vue> [1 x 1]\n# Database: DuckDB 0.9.0 [unknown@Linux 6.2.0-36-generic:R 4.3.2/ma_base.db]\n  trip_distance\n          <dbl>\n1          4.65\n\n\n\nne copie pas de données mais temps de traitement potentiellement plus long"
  },
  {
    "objectID": "presentation_insee.html#dbplyr-et-les-fonctions-duckdb",
    "href": "presentation_insee.html#dbplyr-et-les-fonctions-duckdb",
    "title": "parquet, arrow et duckdb",
    "section": "dbplyr et les fonctions duckdb",
    "text": "dbplyr et les fonctions duckdb\ndbplyr transmet les fonctions qu’il ne comprends pas à la base :\n\n\n\n\nresult <- tbl(con, \"une_table\") |>\n  mutate(\n    # [^a-z0-9 ] indique tous les caractères SAUF ceux cités (et ceux entre les deux bornes)\n    # le 'g' final indique que le replacement doit être global\n    adresse = REGEXP_REPLACE(LOWER(adresse), '[^a-z0-9 ]+', '', 'g'),\n    # on peut mixer les fonctions dplyr avec les fonctions de duckdb\n    annee_extraite = as.integer(REGEXP_EXTRACT(nom_de_fichier, '([0-9]+)')),\n    premier_janvier = MAKE_DATE(annee_extraite, 1L, 1L),\n    premier_du_mois = DATE_TRUNC('month', date)\n  )"
  },
  {
    "objectID": "presentation_insee.html#dbplyr-et-les-fonctions-duckdb-2",
    "href": "presentation_insee.html#dbplyr-et-les-fonctions-duckdb-2",
    "title": "parquet, arrow et duckdb",
    "section": "dbplyr et les fonctions duckdb (2)",
    "text": "dbplyr et les fonctions duckdb (2)\n\ntbl(con, \"une_table\")\n\n# Source:   table<une_table> [2 x 3]\n# Database: DuckDB 0.9.0 [unknown@Linux 6.2.0-36-generic:R 4.3.2/:memory:]\n  adresse                            nom_de_fichier          date      \n  <chr>                              <chr>                   <date>    \n1 1, RUE DE BERCY, 92500 Paris       fichier_1993.csv        2023-09-10\n2 3 rue de bagnolet. 93000 Montreuil nom_de_fichier_2003.csv 2019-03-05\n\n\n\nresult |>\n  select(-c(\"nom_de_fichier\", \"date\")) |>\n  collect()\n\n# A tibble: 2 × 4\n  adresse                         annee_extraite premier_janvier premier_du_mois\n  <chr>                                    <int> <date>          <date>         \n1 1 rue de bercy 92500 paris                1993 1993-01-01      2023-09-01     \n2 3 rue de bagnolet 93000 montre…           2003 2003-01-01      2019-03-01"
  },
  {
    "objectID": "presentation_insee.html#dbplyr-et-les-fonctions-duckdb-3",
    "href": "presentation_insee.html#dbplyr-et-les-fonctions-duckdb-3",
    "title": "parquet, arrow et duckdb",
    "section": "dbplyr et les fonctions duckdb (3)",
    "text": "dbplyr et les fonctions duckdb (3)\nCette fonctionnalité permet de réutiliser directement l’ensemble des fonctions intégrées à duckdb :\n\nles fonctions de chaines de caractères\nles fonctions de calcul sur les dates\nles fonctions d’extraction d’élements de dates\nles fonctions de “pattern matching”\n…"
  },
  {
    "objectID": "presentation_insee.html#générer-des-fichiers-intermédiaires-volumineux",
    "href": "presentation_insee.html#générer-des-fichiers-intermédiaires-volumineux",
    "title": "parquet, arrow et duckdb",
    "section": "Générer des fichiers intermédiaires (volumineux)",
    "text": "Générer des fichiers intermédiaires (volumineux)\nBeaucoup de méthodes possibles, mais plusieurs ne sont pas optimisées."
  },
  {
    "objectID": "presentation_insee.html#générer-des-fichiers-intermédiaires-1",
    "href": "presentation_insee.html#générer-des-fichiers-intermédiaires-1",
    "title": "parquet, arrow et duckdb",
    "section": "Générer des fichiers intermédiaires (1)",
    "text": "Générer des fichiers intermédiaires (1)\nEn utilisant arrow::to_arrow() pour passer la requête de dbplyrà arrow :\n\ntbl(con, glue::glue(\"read_parquet('{taxi_dir}/**/*.parquet')\")) |>\n  filter(year == 2016, month == 1) |>\n  mutate(wday = wday(pickup_datetime)) |>\n  to_arrow() |>\n  write_dataset(tempfile())"
  },
  {
    "objectID": "presentation_insee.html#générer-des-fichiers-intermédiaires-2",
    "href": "presentation_insee.html#générer-des-fichiers-intermédiaires-2",
    "title": "parquet, arrow et duckdb",
    "section": "Générer des fichiers intermédiaires (2)",
    "text": "Générer des fichiers intermédiaires (2)\nEn utilisant uniquement arrow :\n\nopen_dataset(taxi_dir) |>\n  filter(year == 2016, month == 1) |>\n  mutate(wday = wday(pickup_datetime)) |>\n  write_dataset(tempfile())"
  },
  {
    "objectID": "presentation_insee.html#générer-des-fichiers-intermédiaires-3",
    "href": "presentation_insee.html#générer-des-fichiers-intermédiaires-3",
    "title": "parquet, arrow et duckdb",
    "section": "Générer des fichiers intermédiaires (3)",
    "text": "Générer des fichiers intermédiaires (3)\nEn passant par une requête SQL générée à partir de la requête dbplyr :\n\ndplyr_request <- tbl(con, \n                     glue::glue(\"read_parquet('{taxi_dir}/**/*.parquet')\")) |>\n  filter(year == 2016, month == 1) |>\n  mutate(wday = wday(pickup_datetime))\n\nsql <- dplyr_request |> dbplyr::sql_render(con = con) |> DBI::SQL()\n\ndbExecute(con, glue::glue(\"COPY ({sql}) TO '{tempfile()}' (FORMAT PARQUET)\"))\n\n[1] 10906858"
  },
  {
    "objectID": "presentation_insee.html#générer-des-fichiers-intermédiaires-4",
    "href": "presentation_insee.html#générer-des-fichiers-intermédiaires-4",
    "title": "parquet, arrow et duckdb",
    "section": "Générer des fichiers intermédiaires (4)",
    "text": "Générer des fichiers intermédiaires (4)\n\n\n\nméthode\nmémoire\ntemps\n\n\n\n\ntbl() |> to_arrow() |> write_dataset()\n15Go\n305s\n\n\nopen_dataset() |> write_dataset()\n7Go\n325s\n\n\nCOPY ... TO...\n2Go\n145s\n\n\n\n\nn’utilisez surtout pas collect() et/ou arrow::write_parquet()\nduckdb génère des fichiers souvent moins compressés que arrow (10 à 20%)"
  },
  {
    "objectID": "presentation_insee.html#conversion-de-fichiers-sas",
    "href": "presentation_insee.html#conversion-de-fichiers-sas",
    "title": "parquet, arrow et duckdb",
    "section": "Conversion de fichiers SAS",
    "text": "Conversion de fichiers SAS\n\nPlusieurs outils possibles pour les “petits” fichiers (haven, parquetize)\nPour les gros fichiers readstat\n\nreadstat est fort en conversion mais pas en compression : repassez dessus avec arrow ou duckdb (gain d’un facteur 10)\nattention aux NA spécifiques de SAS parfois convertis en NaN"
  },
  {
    "objectID": "presentation_insee.html#regardez-le-sql-de-duckdb",
    "href": "presentation_insee.html#regardez-le-sql-de-duckdb",
    "title": "parquet, arrow et duckdb",
    "section": "Regardez le SQL de duckdb",
    "text": "Regardez le SQL de duckdb\n\nun SQL “rajeuni”\ndes ordres et fonctions orientées statistiques\n\nAvec duckdb, SQL devient un langage vraiment pertinent pour vous : un exemple"
  },
  {
    "objectID": "presentation_insee.html#quel-format-de-fichier-choisir",
    "href": "presentation_insee.html#quel-format-de-fichier-choisir",
    "title": "parquet, arrow et duckdb",
    "section": "Quel format de fichier choisir",
    "text": "Quel format de fichier choisir\n\nprivilégiez parquet\nN’utilisez pas le format natif de duckdb pour des données “pérennes”."
  },
  {
    "objectID": "presentation_insee.html#comment-choisir-votre-outil",
    "href": "presentation_insee.html#comment-choisir-votre-outil",
    "title": "parquet, arrow et duckdb",
    "section": "Comment choisir votre outil",
    "text": "Comment choisir votre outil\n\nvous n’avez pas de données massives : conservez vos outils actuels\nvous utilisez une seule grosse table : arrow et/ou duckdb\nvous devez faire des jointures entre des grosses tables : duckdb\nvous aimez SQL : duckdb"
  },
  {
    "objectID": "presentation_insee.html#vous-avez-converti-un-traitement",
    "href": "presentation_insee.html#vous-avez-converti-un-traitement",
    "title": "parquet, arrow et duckdb",
    "section": "Vous avez converti un traitement",
    "text": "Vous avez converti un traitement\nFaites des tests :\n\nsur quelques lignes et comparer les deux sorties.\nsur les agrégats entre les deux méthodes\nsur le fonctionnement de vos expressions rationnelles\nsur la conversion des dates par lubridate (calcul de max sur les dates, que donne un NA…)\nsur la présence de NaN si vos fichiers sont convertis de SAS\n…"
  },
  {
    "objectID": "presentation_insee.html#les-limites-des-conseils",
    "href": "presentation_insee.html#les-limites-des-conseils",
    "title": "parquet, arrow et duckdb",
    "section": "Les limites des conseils",
    "text": "Les limites des conseils\nCes outils évoluent vite, certaines limites/recommandations ne seront plus valables dans les prochaines versions de arrow et duckdb."
  },
  {
    "objectID": "presentation_insee.html#fin",
    "href": "presentation_insee.html#fin",
    "title": "parquet, arrow et duckdb",
    "section": "FIN",
    "text": "FIN\nMerci pour les présentations INSEE sur arrow et duckdb de fin 2022"
  },
  {
    "objectID": "presentation_insee.html#quelques-références",
    "href": "presentation_insee.html#quelques-références",
    "title": "parquet, arrow et duckdb",
    "section": "Quelques références",
    "text": "Quelques références\n\nla documentation de l’API R de duckdb\nla documentation de dbplyr\nla documentation du SQL de duckdb\nles pages sur parquet et sur l’accès aux bases de données d’utilitR"
  },
  {
    "objectID": "introduction-donnees-massives-arrow.html",
    "href": "introduction-donnees-massives-arrow.html",
    "title": "Traiter des données massives en R avec arrow",
    "section": "",
    "text": "Avertissement\n\n\n\nChapitre en construction\nL’objectif de ce document est de voir comment traiter des données massives avec :\nCes outils ne sont pas magiques, ils permettent :\nMais ils ont leurs limites aussi.\nPar ailleurs, le présentateur a beaucoup expérimenté depuis quelques semaines mais n’est pas data-scientiste et a certainement loupé des choses."
  },
  {
    "objectID": "introduction-donnees-massives-arrow.html#quelques-packages-utilisés",
    "href": "introduction-donnees-massives-arrow.html#quelques-packages-utilisés",
    "title": "Traiter des données massives en R avec arrow",
    "section": "Quelques packages utilisés",
    "text": "Quelques packages utilisés\n\nlibrary(arrow)\nlibrary(tidyverse)\nlibrary(tictoc)\nlibrary(fs)"
  },
  {
    "objectID": "introduction-donnees-massives-arrow.html#travail-sur-une-table-unique",
    "href": "introduction-donnees-massives-arrow.html#travail-sur-une-table-unique",
    "title": "Traiter des données massives en R avec arrow",
    "section": "Travail sur une table unique",
    "text": "Travail sur une table unique\nDans la suite de cette documentation, nous allons utiliser les données des courses de taxi new-yorkais diffusées par la ville de New York qui sont déjà disponibles sur le partage accessible depuis les serveurs R. Ce jeux de données fait 64Go et contient un peu plus d’1 milliard 600 millions de lignes.\n\ntaxi_dir <- \"/nfs/partage-r-sas/exemples/taxis-data/nyc-taxi\"\n\n\n\n\nPour accéder aux données, nous allons utiliser arrow::open_dataset() :\n\nnyc_taxi <- arrow::open_dataset(taxi_dir)\n\nnyc_taxi\n\nFileSystemDataset with 158 Parquet files\nvendor_name: string\npickup_datetime: timestamp[ms]\ndropoff_datetime: timestamp[ms]\npassenger_count: int64\ntrip_distance: double\npickup_longitude: double\npickup_latitude: double\nrate_code: string\nstore_and_fwd: string\n...\n\n\nCette fonction ne retourne pas le dataframe habituel mais un certain nombre d’informations comme le nombre de fichiers et le nom des champs.\nD’après les informations, les données sont réparties dans 158 fichiers différents :\n\nfs::dir_tree(taxi_dir)\n\n/home/nc/travail/R/Rexploration/rdata/nyc-taxi/\n├── year=2009\n│   ├── month=1\n│   │   └── part-0.parquet\n│   ├── month=10\n│   │   └── part-0.parquet\n│   ├── month=11\n│   │   └── part-0.parquet\n│   ├── month=12\n│   │   └── part-0.parquet\n...\n\n\nLes fichiers sont répartis dans des sous-répertoires par années et par mois, nous verrons plus tard l’intérêt de cette organisation.\n\nExemple 1 : interagir avec les données\nRegardons les premières lignes des données avec la fonction head() :\n\nnyc_taxi %>%\n  head()\n\nTable\n6 rows x 24 columns\n$vendor_name <string>\n$pickup_datetime <timestamp[ms]>\n$dropoff_datetime <timestamp[ms]>\n...\n\n\nComme pour open_dataset(), head() ne retourne pas les données elles-mêmes mais uniquement la liste des champs. Avec arrow, aucune opération n’est réalisée à l’ouverture d’un fichier, pour déclencher les calculs, vous devez utiliser la fonction collect() :\n\nnyc_taxi %>%\n  head() %>%\n  collect()\n\n# A tibble: 6 × 24\n  vendor_name pickup_datetime     dropoff_datetime    passenger_count\n  <chr>       <dttm>              <dttm>                        <int>\n1 VTS         2009-01-04 03:52:00 2009-01-04 04:02:00               1\n2 VTS         2009-01-04 04:31:00 2009-01-04 04:38:00               3\n3 VTS         2009-01-03 16:43:00 2009-01-03 16:57:00               5\n4 DDS         2009-01-01 21:52:58 2009-01-01 22:14:00               1\n5 DDS         2009-01-24 17:18:23 2009-01-24 17:24:56               1\n6 DDS         2009-01-16 23:35:59 2009-01-16 23:43:35               2\n# ℹ 20 more variables: trip_distance <dbl>, pickup_longitude <dbl>,\n#   pickup_latitude <dbl>, rate_code <chr>, store_and_fwd <chr>,\n#   dropoff_longitude <dbl>, dropoff_latitude <dbl>, payment_type <chr>,\n#   fare_amount <dbl>, extra <dbl>, mta_tax <dbl>, tip_amount <dbl>,\n#   tolls_amount <dbl>, total_amount <dbl>, improvement_surcharge <dbl>,\n#   congestion_surcharge <dbl>, pickup_location_id <int>,\n#   dropoff_location_id <int>, year <int>, month <int>\n\n\nCette façon de faire permet à la librairie arrow d’avoir une vision globale des calculs qu’elle va devoir réaliser et d’optimiser les opérations dans leur ensemble.\n\n\nExemple 2 : une requête simple\nNous voulons calculer sur 2017, 2019 et 2021 le nombre de trajets, de trajets partagés et le pourcentage de trajets sur trajets partagés.\nNous allons utilisez les verbes dplyr classiques :\n\nfilter() pour filtrer sur la période\ngroup_by() pour groupper sur l’année\nsummarize() pour compter le nombre de trajet et de trajets partagés\nmutate() pour calculer le pourcentage\ncollect() pour déclencher l’exécution et retourner le résultat dans R\n\n\nshared_rides <- nyc_taxi |>\n  filter(year %in% c(2017, 2019, 2021)) |> \n  group_by(year) |>\n  summarize(\n    all_trips = n(),\n    shared_trips = sum(passenger_count > 1, na.rm = TRUE)\n  ) |>\n  mutate(pct_shared = shared_trips / all_trips * 100)\n\n\nshared_rides %>%\n  collect()\n\n# A tibble: 3 × 4\n   year all_trips shared_trips pct_shared\n  <int>     <int>        <int>      <dbl>\n1  2017 113495512     32296166       28.5\n2  2019  84393604     23515989       27.9\n3  2021  30902618      7221844       23.4\n\n\nIl faut noter que, comme précédemment, aucun calcul n’est réalisé en R. arrow va convertir tous ces ordres et les exécuter en interne ce qui implique des limitations qu’il faut comprendre.\n\n\nExemple 3 : Limitations\nL’affichage est illisible, nous souhaitons afficher le résultat par millions de courses.\n\nmillions <- function(x) x / 10^6\n\nNous pouvons utiliser mutate sur chaque variable concernée :\n\nshared_rides |>\n  mutate(\n    all_trips = millions(all_trips),\n    shared_trips = millions(shared_trips)\n  ) |>\n  collect()\n\n# A tibble: 3 × 4\n   year all_trips shared_trips pct_shared\n  <int>     <dbl>        <dbl>      <dbl>\n1  2017     113.         32.3        28.5\n2  2019      84.4        23.5        27.9\n3  2021      30.9         7.22       23.4\n\n\nNous pouvons aussi essayer d’utiliser mutate_at :\n\nshared_rides |>\n  mutate_at(c(\"all_trips\", \"shared_trips\"), millions) |>\n  collect()\n\nError in `map_lgl()`:\nℹ In index: 1.\nCaused by error in `if (deparse(expr[[1]]) == name) ...`:\n! the condition has length > 1\n\n\nMais certains verbes ne sont pas (encore ?) implémentés dans arrow. Une façon de faire est alors de réaliser le calcul lourd dans arrow et, une fois que le volume de données est suffisament réduit d’appliquer la méthode mutate_at :\n\nshared_rides |>\n  collect() |>\n  mutate_at(c(\"all_trips\", \"shared_trips\"), millions)\n\n# A tibble: 3 × 4\n   year all_trips shared_trips pct_shared\n  <int>     <dbl>        <dbl>      <dbl>\n1  2017     113.         32.3        28.5\n2  2019      84.4        23.5        27.9\n3  2021      30.9         7.22       23.4\n\n\nLa liste complète des verbes implémentés est disponible dans la documentation de arrow.\nNous voyons que si mutate_at() n’est pas implémenté, mutate() et across() le sont, nous pourrions donc aussi écrire :\n\nshared_rides |>\n  mutate(across(ends_with(\"trips\"), millions)) |>\n  collect()\n\n# A tibble: 3 × 4\n   year all_trips shared_trips pct_shared\n  <int>     <dbl>        <dbl>      <dbl>\n1  2017     113.         32.3        28.5\n2  2019      84.4        23.5        27.9\n3  2021      30.9         7.22       23.4\n\n\n\n\nQuelques détails pertinents sur le format parquet\nAvant d’aller plus loin, revenons sur l’organisation des fichiers parquet. Pour cela, testons les performances de 3 requêtes avec des différents niveaux de filtres :\n\ntic()\nopen_dataset(taxi_dir) %>%\n  summarize(n = n(), mean = mean(trip_distance)) %>%\n  collect() %>% invisible()\ntoc()\n\n17.053 sec elapsed\n\ntic()\nopen_dataset(taxi_dir) %>% \n  filter(trip_distance > 200) %>% \n  summarize(n = n(), mean = mean(trip_distance)) %>% \n  collect() %>% invisible()\ntoc()\n\n4.274 sec elapsed\n\ntic()\nopen_dataset(taxi_dir) %>% \n  filter(year == 2016, month == 3) %>% \n  summarize(n = n(), mean = mean(trip_distance)) %>%\n  collect() %>% invisible()\ntoc()\n\n2.235 sec elapsed\n\n\nPour comprendre ces différences de performance il faut se rappeler que :\nles données parquet sont organisées dans plusieurs fichiers :\n\nfs::dir_tree(taxi_dir)\n\n/home/nc/travail/R/Rexploration/rdata/nyc-taxi/\n├── year=2009\n│   ├── month=1\n│   │   └── part-0.parquet\n│   ├── month=10\n│   │   └── part-0.parquet\n│   ├── month=11\n│   │   └── part-0.parquet\n│   ├── month=12\n│   │   └── part-0.parquet\n...\n\n\nEt savoir que le format parquet est organisé en colonne et non en ligne comme CSV :\n\net enfin que chaque fichier intègre des méta-données comme les minimums et maximums pour chaque colonne numérique.\nPour les trois requêtes précédentes :\n\nDans le premier cas, arrow lit tous les fichiers mais uniquement la colonne trip_distance.\nDans le deuxième cas, arrow lit\n\nles métadonnées des fichiers et sélectionne ceux où le minimum de trip_distance est supérieur à 200\nla colonne trip_distance uniquement dans ces fichiers\n\nDans le dernier cas arrow lit uniquement la colonne trip_distance des fichiers du répertoire year=2016/month=1/\n\n\n\n\n\n\n\nAstuce\n\n\n\n\nparquet permet facilement de lire uniquement certaines colonnes\nles métadonnées apportent un gain direct, sans action de votre part\nsi vos données sont volumineuses, il est fortement recommandé de partitionner\nil y a dans tous les cas peu de perte à partitionner, au pire ça ne sert à rien.\n\n\n\n\n\nlire des données “non parquet”\narrow permet évidemment de manipuler des données “non parquet”.\nPour un fichier CSV, on peut aussi utiliser la fonction arrow::read_csv_arrow() avec l’argument as_data_frame à FALSE :\n\nread_csv_arrow(file.path(taxi_dir, \"../taxi+_zone_lookup.csv\"),\n               as_data_frame = FALSE)\n\nTable\n265 rows x 4 columns\n$LocationID <int64>\n$Borough <string>\n$Zone <string>\n$service_zone <string>\n\n\nLa première solution permet de convertir n’importe quel data.frame/tibble :\n\ncharger les données dans un data.frame\nconvertir le data.frame au format arrow\n\n\nreadr::read_csv(file.path(taxi_dir, \"../taxi+_zone_lookup.csv\")) %>%\n  as_arrow_table()\n\nRows: 265 Columns: 4\n── Column specification ────────────────────────────────────────────────────────\nDelimiter: \",\"\nchr (3): Borough, Zone, service_zone\ndbl (1): LocationID\n\nℹ Use `spec()` to retrieve the full column specification for this data.\nℹ Specify the column types or set `show_col_types = FALSE` to quiet this message.\n\n\nTable\n265 rows x 4 columns\n$LocationID <double>\n$Borough <string>\n$Zone <string>\n$service_zone <string>\n\nSee $metadata for additional Schema metadata\n\n\n\n\nExemple 4 : au delà de dplyr\narrow implémente également des verbes de lubridate et stringr.\nNous pouvons par exemple calculer la moyenne du nombre de passagers par jour de la semaine et par année :\n\nopen_dataset(taxi_dir) %>%\n  mutate(weekday = wday(pickup_datetime, label = FALSE)) %>%\n  group_by(year, weekday) %>%\n  summarize(moyenne_pass = mean(passenger_count, na.rm = TRUE)) %>%\n  collect() %>%\n  arrange(year, weekday) %>%\n  pivot_wider(names_from = weekday, values_from = moyenne_pass)\n\n# A tibble: 14 × 8\n# Groups:   year [14]\n    year   `1`   `2`   `3`   `4`   `5`   `6`   `7`\n   <int> <dbl> <dbl> <dbl> <dbl> <dbl> <dbl> <dbl>\n 1  2009  1.79  1.63  1.64  1.63  1.65  1.69  1.81\n 2  2010  1.75  1.63  1.62  1.63  1.64  1.68  1.78\n 3  2011  1.72  1.61  1.60  1.60  1.62  1.65  1.75\n 4  2012  1.75  1.65  1.65  1.65  1.66  1.69  1.78\n 5  2013  1.77  1.67  1.68  1.68  1.68  1.71  1.78\n 6  2014  1.74  1.67  1.66  1.67  1.67  1.69  1.76\n 7  2015  1.73  1.66  1.65  1.65  1.66  1.68  1.74\n 8  2016  1.71  1.63  1.63  1.63  1.63  1.65  1.72\n 9  2017  1.67  1.61  1.60  1.60  1.60  1.62  1.68\n10  2018  1.64  1.58  1.57  1.57  1.57  1.59  1.66\n11  2019  1.62  1.54  1.53  1.53  1.53  1.56  1.64\n12  2020  1.52  1.44  1.44  1.44  1.44  1.46  1.54\n13  2021  1.48  1.41  1.39  1.40  1.40  1.43  1.50\n14  2022  1.46  1.37  1.35  1.35  1.35  1.39  1.47"
  },
  {
    "objectID": "introduction-donnees-massives-arrow.html#et-les-jointures-dans-tout-ça",
    "href": "introduction-donnees-massives-arrow.html#et-les-jointures-dans-tout-ça",
    "title": "Traiter des données massives en R avec arrow",
    "section": "Et les jointures dans tout ça ?",
    "text": "Et les jointures dans tout ça ?\nLes courses de taxi intègre une colonne pickup_location_id et dropoff_location_id et le fichier taxi+_zone_lookup.csv donne la correspondance entre cet identifiant et le nom.\n\nExemple : attention aux pièges\nChargeons le fichier en faisant un peu de nettoyage sur les noms de variables :\n\ntaxi_zones <- readr::read_csv(file.path(taxi_dir, \"../taxi+_zone_lookup.csv\")) %>%\n  select(\n    pickup_location_id = LocationID,\n    borough = Borough\n  )\n\nRows: 265 Columns: 4\n── Column specification ────────────────────────────────────────────────────────\nDelimiter: \",\"\nchr (3): Borough, Zone, service_zone\ndbl (1): LocationID\n\nℹ Use `spec()` to retrieve the full column specification for this data.\nℹ Specify the column types or set `show_col_types = FALSE` to quiet this message.\n\ntaxi_zones\n\n# A tibble: 265 × 2\n   pickup_location_id borough      \n                <dbl> <chr>        \n 1                  1 EWR          \n 2                  2 Queens       \n 3                  3 Bronx        \n 4                  4 Manhattan    \n 5                  5 Staten Island\n 6                  6 Staten Island\n 7                  7 Queens       \n 8                  8 Queens       \n 9                  9 Queens       \n10                 10 Queens       \n# ℹ 255 more rows\n\n\nEssayons de faire la jointure sur la colonne pickup_location_id :\n\nopen_dataset(taxi_dir) %>%\n  left_join(taxi_zones) |>\n  collect()\n\nError in `compute.arrow_dplyr_query()`:\n! Invalid: Incompatible data types for corresponding join field keys: FieldRef.Name(pickup_location_id) of type int64 and FieldRef.Name(pickup_location_id) of type double\n\n\narrow est pointilleux sur les types de données et en l’occurence, pickup_location_id est de type int64 dans la les courses et de type double dans la table des zones.\nPour contourner ce piège, nous allons devoir passer un schéma pour forcer le type de LocationID :\n\ntaxi_zones <- readr::read_csv(file.path(taxi_dir, \"../taxi+_zone_lookup.csv\")) %>%\n  select(\n    location_id = LocationID,\n    borough = Borough\n  ) |>\n  as_arrow_table(\n    schema = schema(\n      location_id = int64(),\n      borough = utf8()\n    )\n  )\n\nRows: 265 Columns: 4\n── Column specification ────────────────────────────────────────────────────────\nDelimiter: \",\"\nchr (3): Borough, Zone, service_zone\ndbl (1): LocationID\n\nℹ Use `spec()` to retrieve the full column specification for this data.\nℹ Specify the column types or set `show_col_types = FALSE` to quiet this message.\n\n\nA partir de cet objet arrow, nous allons créer deux objets dédiés à la jointure sur la zone de prise en charge et de dépose :\n\npickup <- taxi_zones |>\n  select(\n    pickup_location_id = location_id,\n    pickup_borough = borough\n  )\n\ndropoff <- taxi_zones |>\n  select(\n    dropoff_location_id = location_id,\n    dropoff_borough = borough\n  )\n\nEt on peut maintenant utiliser ces objets directement :\n\ntic()\nborough_counts <- nyc_taxi |>\n  filter(year == 2016) |>\n  left_join(pickup) |>\n  left_join(dropoff) |>\n  count(pickup_borough, dropoff_borough) |>\n  arrange(desc(n)) |>\n  collect()\n\nborough_counts\n\n# A tibble: 50 × 3\n   pickup_borough dropoff_borough        n\n   <chr>          <chr>              <int>\n 1 <NA>           <NA>            69406520\n 2 Manhattan      Manhattan       51485239\n 3 Manhattan      Queens           2144638\n 4 Queens         Manhattan        2045826\n 5 Manhattan      Brooklyn         1922556\n 6 Queens         Queens           1013587\n 7 Unknown        Unknown           769849\n 8 Brooklyn       Brooklyn          648109\n 9 Queens         Brooklyn          503246\n10 Manhattan      Bronx             289341\n# ℹ 40 more rows\n\ntoc()\n\n12.123 sec elapsed"
  },
  {
    "objectID": "introduction-donnees-massives-arrow.html#quand-arrow-ne-sait-pas-faire-le-job",
    "href": "introduction-donnees-massives-arrow.html#quand-arrow-ne-sait-pas-faire-le-job",
    "title": "Traiter des données massives en R avec arrow",
    "section": "Quand arrow ne sait pas faire le job",
    "text": "Quand arrow ne sait pas faire le job\nNous avons vu que arrow n’avait pas une couverture complète des verbes de dplyr. Certaines fonctions sont implémentées de façon “partielle” (par exemple la médiane est une approximation) tandis que d’autres ne le sont pas du tout comme les fonctions de fenêtre (row_number, …).\nPrenons un exemple (totalement absurde) : on recherche, sur l’année 2018, toutes les courses dont le “rang” contient 59, qui ont commencé à la minute 59 et la seconde 59 et nous voulons extraire le “magic_number” qui supprime tous les chiffres autre que 5 et 9 du rang de la course :\n\nnyc_taxi_2018 <- open_dataset(file.path(taxi_dir, \"year=2018/\"))\n\nnyc_taxi_2018 %>% nrow()\n\n[1] 102797401\n\n\nCes données font plus de 100 millions de lignes, ce qui commence à être compliqué à traiter avec dplyr. Essayons avec arrow :\n\nnyc_taxi_2018 %>%\n  mutate(trip_id = row_number()) |>\n  filter(\n    trip_id |> as.character() |> str_detect(\"59\"),\n    second(pickup_datetime) == 59,\n    minute(pickup_datetime) == 59\n  ) |> \n  mutate(\n    magic_number = trip_id |> \n      as.character() |> \n      str_remove_all(\"[^59]\") |>\n      as.integer()\n  ) |>\n  select(trip_id, magic_number, pickup_datetime) |>\n  collect()\n\nError: Expression row_number() not supported in Arrow\nCall collect() first to pull data into R.\n\n\narrow, très poliment, nous explique que row_number() n’est pas supporté, une solution dans ce cas là est d’utiliser les librairies duckdb et dbplyr :\n\nlibrary(duckdb)\nlibrary(dbplyr)\n\narrow et duckdb savent se parler très efficacement avec les fonctions arrow::to_arrow() et arrow::to_duckdb() qui ne passent pas les données proprement dite mais juste un pointeur sur les données arrow :\n\ntic()\nnyc_taxi_2018 %>%\n  to_duckdb() |>  \n  window_order(pickup_datetime) |>  \n  mutate(trip_id = row_number()) |>\n  filter(\n    trip_id |> as.character() |> str_detect(\"59\"),\n    second(pickup_datetime) == 59,\n    minute(pickup_datetime) == 59\n  ) |> \n  mutate(\n    magic_number = trip_id |> \n      as.character() |> \n      str_remove_all(\"[^59]\") |>\n      as.integer()\n  ) |>\n  select(trip_id, magic_number, pickup_datetime) |>\n  collect()\n\n# A tibble: 1,760 × 3\n   trip_id magic_number pickup_datetime    \n     <dbl>        <int> <dttm>             \n 1  159065          595 2018-01-01 15:59:59\n 2  159066           59 2018-01-01 15:59:59\n 3  159067           59 2018-01-01 15:59:59\n 4  159068           59 2018-01-01 15:59:59\n 5  245759          559 2018-01-02 04:59:59\n 6  446590           59 2018-01-02 20:59:59\n 7  446591           59 2018-01-02 20:59:59\n 8  446592           59 2018-01-02 20:59:59\n 9  593933          599 2018-01-03 13:59:59\n10  593934          599 2018-01-03 13:59:59\n# ℹ 1,750 more rows\n\ntoc()\n\n22.8 sec elapsed"
  },
  {
    "objectID": "introduction-donnees-massives-arrow.html#quelques-conclusions-temporaires",
    "href": "introduction-donnees-massives-arrow.html#quelques-conclusions-temporaires",
    "title": "Traiter des données massives en R avec arrow",
    "section": "Quelques conclusions temporaires",
    "text": "Quelques conclusions temporaires\n\nLimitations de arrow\narrow est un outil de manipulation de données qui excelle dans les jointures, filtre, summmarize… Il n’est pas (forcément) adapté aux traitements statistiques tels que régression, tirages pondérés dans un échantillon…\n\n\njointures entre des grosses tables\nSur des « grosses » jointure arrow va essayer de charger en mémoire les données (et généralement planter). Nous verrons plus en profondeur dans une prochaine session l’utilisation de duckdb pour ces cas là.\n\n\narrow n’a pas forcément le même comportement que dplyr\nEt en particulier arrow est plus strict que le tidyverse standard :\n\ncertains “petits trucs sales” en R ne passent pas (sommer des booleens)\nle typage est important, impossible de faire un filter avec un chiffre si le champs est une string.\nla recherche peut poser problème dplyr accepte les regexp comme [:punct:] (qui ne sont normalement pas valides), pour arrow il faudra utiliser [[:punct:]]\nsurtout ne pas réutiliser r-base\n\nQuand vous convertissez un traitement de dplyr vers arrow, faites des tests :\n\nsur quelques lignes et comparer les deux sorties.\nsur les agrégats entre les deux méthodes.\n\nPrécautions : faire des tests sur les fonctions lubridate et stringr implémentée en arrow : essayer sur les ponctuations avec regexp (Attention au conversion de dates avec lubridate récupération d’un NA, calcul de max de date).\n\n\nune librairie encore jeune\narrow est une librairie encore jeune, une nouvelle version sort tous les 2/3 mois avec beaucoup d’évolutions et d’améliorations."
  },
  {
    "objectID": "introduction-donnees-massives-arrow.html#pour-approfondir-arrow",
    "href": "introduction-donnees-massives-arrow.html#pour-approfondir-arrow",
    "title": "Traiter des données massives en R avec arrow",
    "section": "Pour approfondir arrow",
    "text": "Pour approfondir arrow\n\nL’excellent tutorial donné à User2022 (dont je me suis largement inspiré)\nLa documentation officielle de arrow\nLa liste complète des verbes tidyverse supportés par arrow"
  },
  {
    "objectID": "introduction-donnees-massives-arrow.html#la-suite",
    "href": "introduction-donnees-massives-arrow.html#la-suite",
    "title": "Traiter des données massives en R avec arrow",
    "section": "La suite",
    "text": "La suite\nprochaine séance dédiée à duckdb"
  },
  {
    "objectID": "index.html",
    "href": "index.html",
    "title": "site R",
    "section": "",
    "text": "Quelques documents en vracs sur les outils big data de R"
  },
  {
    "objectID": "conversion-fichier-sas.html",
    "href": "conversion-fichier-sas.html",
    "title": "Convertir des fichiers SAS en parquet",
    "section": "",
    "text": "Ce document décrit les différentes méthodes et problèmes rencontrés dans la conversion des fichiers SAS en parquet.\nIl sera complété au fur et à mesure.\nDans la suite, nous utiliserons le fichier iris.sas7bdat du package haven:\n\nfichier_sas <- system.file('examples', 'iris.sas7bdat', package = 'haven')\n\nfichier_sas\n\n[1] \"/nfs/rstudio-libpath/R/x86_64-pc-linux-gnu-library/4.3.1/haven/examples/iris.sas7bdat\""
  },
  {
    "objectID": "conversion-fichier-sas.html#convertir-un-fichier-sas",
    "href": "conversion-fichier-sas.html#convertir-un-fichier-sas",
    "title": "Convertir des fichiers SAS en parquet",
    "section": "Convertir un fichier SAS",
    "text": "Convertir un fichier SAS\nPlusieurs cas sont possibles :\n\nvotre fichier est chiffré : les outils R/python ne permettent pas de lire des fichiers SAS chiffrés, vous devrez donc le déchiffrer et vous retomberez dans un des cas suivants.\nvotre fichier n’est “pas trop gros” : vous pourrez utiliser les outils classiques de R\nvotre fichier est vraiment gros (plusieurs dizaines de Go), vous devrez passer par des outils spécifiques\n\nDans tous les cas, convertir un fichier SAS est un processus lent. Comptez plusieurs dizaines d’heure pour un fichier de 100Go ou plus.\n\nVotre fichier est “normal”\nVous pouvez utiliser directement le package haven et plus particulièrement la fonction haven::read_sas() :\n\nhaven::read_sas(fichier_sas) |>\n  arrow::write_parquet(tempfile(fileext = \".parquet\"))\n\nVous pouvez bien sûr en profiter pour le partitionner si besoin :\n\ndir <- tempfile(fileext = \"_ds_parquet\")\n\nhaven::read_sas(fichier_sas) |>\n  arrow::write_dataset(dir, partitioning = \"Species\")\n\nlist.files(dir)\n\n[1] \"Species=setosa\" \"Species=versic\" \"Species=virgin\"\n\n\nSi votre fichier est plus volumineux et ne tient pas en mémoire, vous pouvez utiliser parquetize (qui utilise haven) et utiliser l’argument max_memory : parquetize lira votre fichier SAS par morceaux et créera un dataset avec les différents morceaux :\n\nparquetize::table_to_parquet(\n  path_to_file = system.file(\"examples\",\"iris.sas7bdat\", package = \"haven\"),\n  path_to_parquet = tempfile(fileext = \"ds_parquet\"),\n)\n\nReading data...\n\n\nWriting data...\n\n\n✔ Data are available in parquet file under /home/users/nicolas.chuche/.tmp/RtmpIp9Dhn/file10e10449cfe0fds_parquet\n\n\nWriting data...\nReading data...\n✔ The /nfs/rstudio-libpath/R/x86_64-pc-linux-gnu-library/4.3.1/haven/examples/iris.sas7bdat file is available in parquet format under /home/users/nicolas.chuche/.tmp/RtmpIp9Dhn/file10e10449cfe0fds_parquet\nReading data...\n\n\n\n\nVotre fichier est vraiment gros\nSur les fichiers vraiment gros haven tournera très longtemps avant de rendre les armes…\nHeureusement il existe le programme readstat qui permet de convertir les fichiers très volumineux en lisant par bloc de lignes (testé jusqu’à des fichiers de 500Go). La version ‘musl’ est compilé avec la librairie C embarquée, vous pouvez donc la copier sur un serveur linux et l’utiliser directement.\nPour voir le contenu du fichier SAS, vous pouvez utiliser la commande metadata :\n\n$ readstat metadata /nfs/rstudio-libpath/R/x86_64-pc-linux-gnu-library/4.3.1/haven/examples/iris.sas7bdat\nMetadata for the file /nfs/rstudio-libpath/R/x86_64-pc-linux-gnu-library/4.3.1/haven/examples/iris.sas7bdat\n\nRow count: 150\nVariable count: 5\nTable name: IRIS\nTable label:\nFile encoding: WINDOWS-1252\nFormat version: 9\nBitness: 32-bit\nCreation time: 2016-06-08 18:38:50\nModified time: 2016-06-08 18:38:50\nCompression: None\nByte order: Little\nVariable names:\n0: Sepal_Length { type class: Numeric, type: Double, label: , format class: , format: BEST, arrow logical data type: Float64, arrow physical data type: Primitive(\n    Float64,\n) }\n1: Sepal_Width { type class: Numeric, type: Double, label: , format class: , format: BEST, arrow logical data type: Float64, arrow physical data type: Primitive(\n    Float64,\n) }\n2: Petal_Length { type class: Numeric, type: Double, label: , format class: , format: BEST, arrow logical data type: Float64, arrow physical data type: Primitive(\n    Float64,\n) }\n3: Petal_Width { type class: Numeric, type: Double, label: , format class: , format: BEST, arrow logical data type: Float64, arrow physical data type: Primitive(\n    Float64,\n) }\n4: Species { type class: String, type: String, label: , format class: , format: $, arrow logical data type: Utf8, arrow physical data type: Utf8 }\n\nEt pour le convertir, la commande data :\n\n$ readstat data fichier.sas7bdat --output fichier.sas7bdat --format parquet\nWriting parsed data to file /home/n.chuche/fichier.parquet\nWrote 150 rows from file iris.sas7bdat into fichier.parquet\nIn total, wrote 150 rows from file iris.sas7bdat into fichier.parquet\n\nComme indiqué au début de ce document, la conversion d’un fichier volumineux prend du temps, beaucoup de temps, comptez plusieurs jours pour des fichiers vraiment de plusieurs centaines de Go."
  },
  {
    "objectID": "conversion-fichier-sas.html#les-problèmes-rencontrés-à-la-conversion",
    "href": "conversion-fichier-sas.html#les-problèmes-rencontrés-à-la-conversion",
    "title": "Convertir des fichiers SAS en parquet",
    "section": "Les problèmes rencontrés à la conversion",
    "text": "Les problèmes rencontrés à la conversion\nSAS a plusieurs valeurs NA, certaines sont traduites en NaN dans parquet. Votre fichier contiendra donc plein de NaN qui poseront des problèmes dans vos calculs R (NaN étant considéré comme Inf, il n’est pas du tout géré comme NA dans les fonctions mathématique, il est considéré comme Inf…)\nSauf à vouloir, à chacun de vos calculs, remplacer les NaN par des NA, vous devrez réaliser une conversion supplémentaire."
  },
  {
    "objectID": "conversion-fichier-sas.html#recetter-la-conversion",
    "href": "conversion-fichier-sas.html#recetter-la-conversion",
    "title": "Convertir des fichiers SAS en parquet",
    "section": "Recetter la conversion",
    "text": "Recetter la conversion\n\nfaites des agrégats\ncomparer quelques lignes\n\nVérifiez bien vos dates."
  }
]