[
  {
    "objectID": "docs/cas-pratique-traitement-duckdb.html",
    "href": "docs/cas-pratique-traitement-duckdb.html",
    "title": "Cas pratique d’une migration d’un traitement vers duckdb",
    "section": "",
    "text": "Ce cas pratique vient du transport. Il sert à préparer des fichiers d’étude à partir de deux jeux de données :\n\nvehicule : les propriétés d’un véhicule (immat, cylindrée, énergie, date de mise en circulation…) fait 70 millions de lignes\nannee : les évènements liés à un véhicule par année (km parcouru, présence dans le parc… ) fait 490 millions de lignes\n\nEt d’une troisième table de 50 lignes avec la nomenclature des carburants.\nDans le code d’origine, afin de tenir sur le serveur, les données vehicule et annee d’origine sont partitionnées à la main en 10 fichiers véhicules et 10 fichiers années correspondants pour générer 10 fichiers d’étude.\nLe traitement actuel est le suivant :\n\ncharge deux fichiers de données parquet en mémoire avec `arrow::read_parquet()\nfait une jointure entre les deux fichiers de données avec dplyr\najoute des colonnes calculées toujours avec dplyr (année de mise en circulation à partir de la date, la quantité de CO2 émise par tranche…)\nécrit le fichier résultat\n\nNotre cas pratique reprend uniquement la génération d’un fichier d’étude sur les 10.\nDans le suite, nous allons rapidement présenter le code d’origine et voir les différentes évolutions possibles."
  },
  {
    "objectID": "docs/cas-pratique-traitement-duckdb.html#le-code-dorigine",
    "href": "docs/cas-pratique-traitement-duckdb.html#le-code-dorigine",
    "title": "Cas pratique d’une migration d’un traitement vers duckdb",
    "section": "Le code d’origine",
    "text": "Le code d’origine\nLe programme d’origine utilise arrow uniquement pour la lecture des données au format parquet. Les calculs sont réalisés avec dplyr.\n\nlibrary(\"arrow\")\nlibrary(\"dplyr\")\nlibrary(\"readxl\")\nlibrary(\"writexl\")\nlibrary(tictoc)\nlibrary(\"data.table\")\n\n\nchemin_donnees_sources <- \"/home/nc/projet/Parc/\"\n\ngrille_carbu <- subset(\n  read_xlsx(\"/home/nc/projet/Nomenclature/Vehicule/nomenclature_carburant_mai_2022.xlsx\"),\n  select=c(carbu_simpl,carbu_det,energ))\n\n### Boucle pour préparer et exporter toutes les tables data_etude_0 à data_etude_9. ###\n\ndata <- left_join(read_parquet(paste0(chemin_donnees_sources,\"data_annee_VP_0.parquet\"),\n                               col_select = c(\"id\",\"year\",\"parc_01_01\",\"parcm\",\"km\")),\n                  read_parquet(paste0(chemin_donnees_sources,\"data_vehicule_VP_0.parquet\"),\n                               col_select = c(\"id\",\"date_mise_en_cir\",\"energ\",\"co2_theorique\",\n                                              \"puis_kw\",\"poids_vide\",\"crit_air\",\"conso_reelle\",\n                                              \"co2_reel\"))\n                  ,by=\"id\") %>%\n  filter(year>=2012) %>%\n  mutate(\n    poids_vide_tr = cut(poids_vide,\n                        breaks=c(min(poids_vide,na.rm=TRUE),1000,1200,1500,1800,2000,\n                                 max(poids_vide,na.rm=TRUE)),\n                        include.lowest=TRUE,\n                        labels=c(\"1 - moins de 1000 kg\",\n                                 \"2 - plus de 1 à 1,2 t\",\n                                 \"3 - plus de 1,2 à 1,5 t\",\n                                 \"4 - plus de 1,5 à 1,8 t\",\n                                 \"5 - plus de 1,8 à 2 t\",\n                                 \"6 - plus de 2 t\")),\n    annee_mise_en_cir = as.numeric(substr(date_mise_en_cir,1,4)),\n    age = as.numeric(difftime(as.IDate(paste0(year,\"-01-01\")),date_mise_en_cir,units=\"days\")/365.25),\n    age_tr = cut(age,\n                 breaks=c(min(age,na.rm=TRUE),2,5,10,15,20,25,max(age,na.rm=TRUE)),\n                 include.lowest=TRUE,\n                 labels=c(\"1 - moins de 2 ans\",\n                          \"2 - plus de 2 à 5 ans\",\n                          \"3 - plus de 5 à 10 ans\",\n                          \"4 - plus de 10 à 15 ans\",\n                          \"5 - plus de 15 à 20 ans\",\n                          \"6 - plus de 20 à 25 ans\",\n                          \"7 - plus de 25 ans\")),\n    co2_reel = as.numeric(co2_reel),\n    co2_reel_tr = cut(co2_reel,\n                      breaks=c(min(co2_reel,na.rm=TRUE),125,150,175,200,max(co2_reel,na.rm=TRUE)),\n                      include.lowest=TRUE,\n                      labels=c(\"1 - moins de 125 g/km\",\n                               \"2 - plus de 125 à 150 g/km\",\n                               \"3 - plus de 150 à 175 g/km\",\n                               \"4 - plus de 175 à 200 g/km\",\n                               \"5 - plus de 200 g/km\")),\n    co2_reel_tr = ifelse(crit_air==\"E\",\"0 - véhicule électrique\",co2_reel_tr),\n    co2_reel_tr = ifelse(is.na(co2_reel_tr),\"NA - inconnu\",co2_reel_tr),\n    co2_theorique = as.numeric(co2_theorique),\n    co2_theorique_tr = cut(co2_theorique,\n                           breaks=c(min(co2_theorique,na.rm=TRUE),125,150,175,200,\n                                    max(co2_theorique,na.rm=TRUE)),\n                           include.lowest=TRUE,\n                           labels=c(\"1 - moins de 125 g/km\",\n                                    \"2 - plus de 125 à 150 g/km\",\n                                    \"3 - plus de 150 à 175 g/km\",\n                                    \"4 - plus de 175 à 200 g/km\",\n                                    \"5 - plus de 200 g/km\")),\n    co2_theorique_tr = ifelse(crit_air==\"E\",\"0 - véhicule électrique\",co2_theorique_tr),\n    co2_theorique_tr = ifelse(is.na(co2_theorique_tr),\"NA - inconnu\",co2_theorique_tr),\n    puis_kw_tr = cut(puis_kw,\n                     breaks=c(min(puis_kw,na.rm=TRUE),55,70,85,100,max(puis_kw,na.rm=TRUE)),\n                     include.lowest=TRUE,\n                     labels=c(\"1 - moins de 55 kw\",\n                              \"2 - plus de 55 à 70 kw\",\n                              \"3 - plus de 70 à 85 kw\",\n                              \"4 - plus de 85 à 100 kw\",\n                              \"5 - plus de 100 kw\")),\n    puis_kw_tr = ifelse(is.na(puis_kw_tr),\"NA - inconnu\",puis_kw_tr)\n  ) %>%\n\n  left_join(grille_carbu,by=\"energ\") %>%\n\n  mutate(\n    carbu_agreg = ifelse(carbu_det %in% c(\"Diesel\",\"Biodiesel\",\"Diesel HNR\",\"Diesel HR\"),\n                         \"Diesel (y compris hybrides)\",\n                         \"Essence et autres énergies\"),\n    carbu_agreg2 = ifelse(carbu_simpl %in% c(\"Diesel\",\"Essence\"),\n                          \"Diesel et Essence thermiques\",\n                          \"Autres énergies\")\n  )\n\nwrite_parquet(data, \"origine.parquet\")\n\n\n\n\n\n\n\nNote\n\n\n\nLe code fonctionne parfaitement et le temps de traitement est de l’ordre de 15min pour une consommation en pic 25Go de mémoire.\n\n\nIl pourrait certainement être optimisé, le mutate réalisé à la fin pourrait l’être directement sur la table grille_carbu au début pour éviter de devoir modifier les millions de lignes mais ça restera de l’optimisation à la marge (test fait, le traitement passe à 12min).\nLes principaux problèmes de ce code, (écrit à une époque ou arrow n’était pas encore vraiment utilisable sur nos serveurs) sont que :\n\nles données sont entièrement chargées en mémoire par arrow::read_parquet()\nles calculs sont réalisés par dplyr qui n’est pas connu pour être rapide\n\nSi ça ne pose pas de problème sur quelques milliers voire centaines de milliers de lignes, ça devient rédhibitoire quand on croise des dizaines de millions de lignes."
  },
  {
    "objectID": "docs/cas-pratique-traitement-duckdb.html#première-étape-utilisation-de-duckdb-et-dplyr",
    "href": "docs/cas-pratique-traitement-duckdb.html#première-étape-utilisation-de-duckdb-et-dplyr",
    "title": "Cas pratique d’une migration d’un traitement vers duckdb",
    "section": "Première étape : utilisation de duckdb et dplyr",
    "text": "Première étape : utilisation de duckdb et dplyr\nDans cette première étape, nous allons essayer d’éviter de charger les données dans R et, surtout, de faire la jointure en mémoire. Pour cela nous allons lire les données avec duckdb et les traiter avec dplyr qui, en sous-main, va utiliser dbplyr (la version de dplyr pour les bases de données).\n\nL’initialisation\nNous allons commencer par charger les packages nécessaires et on créé une variable avec les chemins :\n\nlibrary(arrow)\nlibrary(dplyr)\nlibrary(readxl)\nlibrary(duckdb)\nlibrary(tidyverse)\nlibrary(lubridate)\nlibrary(tictoc)\n\nchemin_donnees_sources <- \"/home/nc/projet/Parc/Parc_2022/Donnees_individuelles/\"\n\nPuis nous créons une base de données dans le fichier test.duckdb, on limite la mémoire à 10Go et le nombre de CPU utilisé à 4 pour ne pas écrouler le serveur.\n\ncon <- DBI::dbConnect(duckdb(), dbdir = \"test.duckdb\",            \n                      config=list(\"memory_limit\"=\"10GB\",\n                                  \"threads=4\"))\n\n\n\nCréation des tables de base de données à utiliser\nLe référentiel des carburants n’étant pas un fichier parquet, on écrit les données dans la base avec la commande DBI::dbWriteTable()\nOn créé les objets vehicule, annee et grille_carbu avec dplyr::tbl(), cette fonction transforme une table de base de données ou un fichier parquet en objet utilisable par dplyr.\n\nread_xlsx(\"/home/nc/projet/Nomenclature/Vehicule/nomenclature_carburant_mai_2022.xlsx\") |>\n  select(carbu_simpl,carbu_det,energ) |>\n  dbWriteTable(con, \"grille_carbu\", value = _, overwrite = TRUE)\ngrille_carbu <- tbl(con, \"grille_carbu\")\n\nvehicule <- tbl(con, file.path(chemin_donnees_sources, \"data_vehicule_VP_0.parquet\")) %>%\n  select(id, date_mise_en_cir,energ, co2_theorique,puis_kw,poids_vide,crit_air,conso_reelle,co2_reel)\n\nannee <- tbl(con, file.path(chemin_donnees_sources, \"data_annee_VP_0.parquet\")) %>%\n  select(id, year, parc_01_01, parcm, km)\n\n\n\nCréation de la requête dbplyr\nJ’ai pris le parti de modifier le code d’origine à minima pour qu’il fonctionne avec dbplyr, les modifications sont à trois niveaux :\n\nchanger les bornes des cut pour les passer à 0 et Inf plutôt que de calculer les min et max\nutiliser lubridate::year() plutôt que as.numeric(substr(date_mise_en_cir,1,4)) (ligne 5)\nutiliser lubridate::as_date() pour calculer l’age du véhicule (ligne 6)\n\n\nrequest <- vehicule %>%\n  right_join(annee, by = c(\"id\")) %>%\n  filter(year >= 2012) %>%\n  mutate(\n    annee_mise_en_cir = year(date_mise_en_cir),\n    age = (as_date(paste0(year, \"-01-01\")) - date_mise_en_cir ) / 365.25,\n    poids_vide_tr = cut(poids_vide,\n                      breaks=c(0,1000,1200,1500,1800,2000,Inf),\n                      include.lowest=TRUE,\n                      labels=c(\"1 - moins de 1000 kg\",\n                               \"2 - plus de 1 à 1,2 t\",\n                               \"3 - plus de 1,2 à 1,5 t\",\n                               \"4 - plus de 1,5 à 1,8 t\",\n                               \"5 - plus de 1,8 à 2 t\",\n                               \"6 - plus de 2 t\")),\n    age_tr = cut(age,\n                 breaks=c(0,2,5,10,15,20,25,Inf),\n                 include.lowest=TRUE,\n                 labels=c(\"1 - moins de 2 ans\",\n                          \"2 - plus de 2 à 5 ans\",\n                          \"3 - plus de 5 à 10 ans\",\n                          \"4 - plus de 10 à 15 ans\",\n                          \"5 - plus de 15 à 20 ans\",\n                          \"6 - plus de 20 à 25 ans\",\n                          \"7 - plus de 25 ans\")),\n    co2_reel_tr = cut(co2_reel,\n                      breaks=c(0,100,130,140,155,175,200,Inf),\n                      include.lowest=TRUE,\n                      labels=c(\"1 - moins de 123 g/km\",\n                               \"2 - plus de 123 à 138 g/km\",\n                               \"3 - plus de 138 à 165 g/km\",\n                               \"4 - plus de 165 à 190 g/km\",\n                               \"5 - plus de 190 à 213 g/km\",\n                               \"6 - plus de 213 à 226 g/km\",\n                               \"7 - plus de 226 g/km\")),\n    co2_reel_tr = ifelse(crit_air==\"E\",\"0 - véhicule électrique\",co2_reel_tr),\n    co2_reel_tr = ifelse(is.na(co2_reel_tr),\"NA - inconnu\",co2_reel_tr),\n    co2_theorique_tr = cut(co2_theorique,\n                           breaks=c(0,123,138,165,190,213,226,Inf),\n                           include.lowest=TRUE,\n                           labels=c(\"1 - moins de 123 g/km\",\n                                    \"2 - plus de 123 à 138 g/km\",\n                                    \"3 - plus de 138 à 165 g/km\",\n                                    \"4 - plus de 165 à 190 g/km\",\n                                    \"5 - plus de 190 à 213 g/km\",\n                                    \"6 - plus de 213 à 226 g/km\",\n                                    \"7 - plus de 226 g/km\")),\n    co2_theorique_tr = ifelse(crit_air==\"E\",\"0 - véhicule électrique\",co2_theorique_tr),\n    co2_theorique_tr = ifelse(is.na(co2_theorique_tr),\"NA - inconnu\",co2_theorique_tr),\n    puis_kw_tr = cut(puis_kw,\n                     breaks=c(0,55,70,85,100,Inf),\n                     include.lowest=TRUE,\n                     labels=c(\"1 - moins de 55 kw\",\n                              \"2 - plus de 55 à 70 kw\",\n                              \"3 - plus de 70 à 85 kw\",\n                              \"4 - plus de 85 à 100 kw\",\n                              \"5 - plus de 100 kw\"))\n    ) %>%\n  left_join(grille_carbu, by = \"energ\") %>%\n  mutate(\n    carbu_agreg = ifelse(carbu_det %in% c(\"Diesel\",\"Biodiesel\",\"Diesel HNR\",\"Diesel HR\"),\n                         \"Diesel (y compris hybrides)\",\n                         \"Essence et autres énergies\"),\n    carbu_agreg2 = ifelse(carbu_simpl %in% c(\"Diesel\",\"Essence\"),\n                          \"Diesel et Essence thermiques\",\n                          \"Autres énergies\")\n  )\n\nCette commande retourne (quasiment) immédiatement car à ce stade, l’objet request n’est pas encore évalué, on parle de “lazy evaluation”, il contient juste les ordres à passer.\n\n\non exécute la requête\nLa requête est effectivement exécuté au moment de la commande write_parquet(), dbplyr va alors transformer tous les ordres en SQL et les envoyer à la base de données.\n\nrequest %>%\n  to_arrow() %>%\n  write_parquet(\"dbplyr.parquet\")\n\n\n\n\n\n\n\nNote\n\n\n\nCe code prend autour de 150 secondes et consomme au pic 15/16Go de mémoire.\n\n\n\n\n\n\n\n\nAstuce\n\n\n\nLa limite à 10Go de duckdb ne fonctionne visiblement pas, pourquoi ?\nUne partie de la mémoire est utilisée par arrow au moment de la matérialisation et, évidemment, les limites fixées à duckdb ne peuvent pas s’appliquer sur arrow."
  },
  {
    "objectID": "docs/cas-pratique-traitement-duckdb.html#dernière-étape-doptimisation",
    "href": "docs/cas-pratique-traitement-duckdb.html#dernière-étape-doptimisation",
    "title": "Cas pratique d’une migration d’un traitement vers duckdb",
    "section": "Dernière étape d’optimisation",
    "text": "Dernière étape d’optimisation\nPeut-on aller plus loin ? Oui mais pour ça il faut éviter de matérialiser les données.\nIl y a deux solutions :\n\nLa solution arrow::write_dataset()\nLa première solution est d’utiliser arrow::write_dataset() à la place de arrow::write_parquet().\nEn effet, contrairement à write_parquet(), write_dataset() sait lire les données envoyées par la requête dbplyr au fur et à mesure tandis que write_parquet a besoin de la totalité des données pour les écrire.\nChanger les dernières lignes write_dataset() a un impact majeur :\n\nrequest %>%\n  to_arrow() %>%\n  write_dataset(\"dbplyr_parquet\")\n\n\n\n\n\n\n\nNote\n\n\n\nCe code prend autour de 100 secondes et consomme au pic 2Go de mémoire.\n\n\n\n\nLa solution “full” duckdb\nLa seconde solution est d’utiliser la commande native de duckdb pour générer les fichiers parquet : COPY ... TO ...\nSi vous êtes expert en SQL vous pouvez convertir mais ça n’est pas mon cas alors nous allons utiliser la fonction dbplyr::sql_render() et utiliser directement le SQL généré par dbplyr :\n\nsql <- request %>% \n  dbplyr::sql_render(con = con)\n\nLa variable sql contient une requête :\n\n<SQL>\nSELECT\n  *,\n  CASE WHEN (carbu_det IN ('Diesel', 'Biodiesel', 'Diesel HNR', 'Diesel HR')) THEN 'Diesel (y compris hybrides)' WHEN NOT (carbu_det IN ('Diesel', 'Biodiesel', 'Diesel HNR', 'Diesel HR')) THEN 'Essence et autres énergies' END AS carbu_agreg,\n  CASE WHEN (carbu_simpl IN ('Diesel', 'Essence')) THEN 'Diesel et Essence thermiques' WHEN NOT (carbu_simpl IN ('Diesel', 'Essence')) THEN 'Autres énergies' END AS carbu_agreg2\nFROM (\n  SELECT\n    *,\n    CASE\nWHEN (puis_kw <= 55.0) THEN '1 - moins de 55 kw'[...]\n[...]\n\nLe SQL est “brutal” mais :\n\nnous ne sommes pas à un concours de beau code\nl’optimisation c’est le boulot de duckdb\n\nNous allons enfin utiliser le SQL généré pour écrire le fichier :\n\ndbExecute(con, glue::glue(\"COPY ({DBI::SQL(request)}) TO 'test2.parquet'\"))\n\nEt alors ?\n\n\n\n\n\n\nNote\n\n\n\nCe code prend également autour de 100 secondes et consomme 1,7Go de mémoire en pic.\n\n\nOn aurait aussi pu partitionner par exemple sur year si cette variable était beaucoup utilisée pour des filter :\n\ndbExecute(con, glue::glue(\"COPY ({DBI::SQL(request)}) \n                          TO 'dataset_par_annee' (FORMAT PARQUET, PARTITION_BY (year))\"))\n\nOn peut noter également que le fichier parquet généré par duckdb est 20% plus gros que celui de arrow (730Go contre 600).\nNous venons de passer le traitement de 15min et plus de 25Go à 100s et 1,7 ou 2Go. On peut s’arrêter là pour aujourd’hui non ?"
  },
  {
    "objectID": "docs/cas-pratique-traitement-duckdb.html#discussion-sur-la-conversion-de-dplyr-à-dbplyr",
    "href": "docs/cas-pratique-traitement-duckdb.html#discussion-sur-la-conversion-de-dplyr-à-dbplyr",
    "title": "Cas pratique d’une migration d’un traitement vers duckdb",
    "section": "Discussion sur la conversion de dplyr à dbplyr",
    "text": "Discussion sur la conversion de dplyr à dbplyr\nLa conversion de ce traitement en duckdb a demandé un peu de travail essentiellement sur la partie manipulation des dates pour trouver les fonctions lubridate connues de dbplyr.\n\n\n\nDerrière la scène j’ai créé une table table avec les deux colonnes nécessaires que j’ai enregistrée dans duckdb dans la table dates :\n\ndates\n\n# A tibble: 2 × 2\n  date_mise_en_cir  year\n  <date>           <int>\n1 2018-04-02        2022\n2 2020-11-29        2023\n\n\nLe passage :\n\ndates |>\n  mutate(\n    annee_mise_en_cir = as.numeric(substr(date_mise_en_cir, 1, 4)),\n    age = as.numeric(difftime(as.IDate(paste0(year,\"-01-01\")), date_mise_en_cir, units=\"days\")/365.25)\n  )\n\n# A tibble: 2 × 4\n  date_mise_en_cir  year annee_mise_en_cir   age\n  <date>           <int>             <dbl> <dbl>\n1 2018-04-02        2022              2018  3.75\n2 2020-11-29        2023              2020  2.09\n\n\nest traduit en :\n\ntbl(con, \"dates\") |>\n  mutate(annee_mise_en_cir = year(date_mise_en_cir)) |>\n  mutate(age = (as_date(paste0(year, \"-01-01\")) - date_mise_en_cir ) / 365.25)\n\n# Source:   SQL [2 x 4]\n# Database: DuckDB 0.9.0 [unknown@Linux 6.2.0-36-generic:R 4.3.2/:memory:]\n  date_mise_en_cir  year annee_mise_en_cir   age\n  <date>           <int>             <dbl> <dbl>\n1 2018-04-02        2022              2018  3.75\n2 2020-11-29        2023              2020  2.09\n\n\nMais dbplyr a une particularité intéressante, quand il ne comprends pas un ordre, il l’envoie tel quel à la base de données. Nous aurions pu nous appuyer sur ce mécanisme pour utiliser la fonction make_date(year, month, day) de duckdb :\n\ntbl(con, \"dates\") |>\n  mutate(annee_mise_en_cir = year(date_mise_en_cir)) |>\n  mutate(age = (make_date(year, 1L, 1L) - date_mise_en_cir ) / 365.25)\n\n# Source:   SQL [2 x 4]\n# Database: DuckDB 0.9.0 [unknown@Linux 6.2.0-36-generic:R 4.3.2/:memory:]\n  date_mise_en_cir  year annee_mise_en_cir   age\n  <date>           <int>             <dbl> <dbl>\n1 2018-04-02        2022              2018  3.75\n2 2020-11-29        2023              2020  2.09\n\n\nVersion nettement plus facile à car faisant uniquement ce qu’il faut sans substr, paste et cie."
  },
  {
    "objectID": "docs/cas-pratique-traitement-duckdb.html#peut-on-utiliser-uniquement-arrow-sur-ce-traitement",
    "href": "docs/cas-pratique-traitement-duckdb.html#peut-on-utiliser-uniquement-arrow-sur-ce-traitement",
    "title": "Cas pratique d’une migration d’un traitement vers duckdb",
    "section": "Peut-on utiliser uniquement arrow sur ce traitement ?",
    "text": "Peut-on utiliser uniquement arrow sur ce traitement ?\nBien sûr, comme pour le point précédent, il y a un travail sur la conversion car arrow :\n\nne gère pas les calculs sur les dates comme dbplyr\nne connait pas base::cut() qui est massivement employé.\n\nSur le premier point, une seule ligne à changer par rapport à notre code dbplyr :\n\ndates %>%\n  mutate(\n    annee_mise_en_cir = year(date_mise_en_cir),\n    age = (as_date(paste0(year, \"-01-01\")) - date_mise_en_cir ) / 365.25\n  )\n\n# A tibble: 2 × 4\n  date_mise_en_cir  year annee_mise_en_cir age          \n  <date>           <int>             <dbl> <drtn>       \n1 2018-04-02        2022              2018 3.750856 days\n2 2020-11-29        2023              2020 2.088980 days\n\n\nArrow n’autorise pas à faire un calcul sur une différence entre deux dates, il faut donc les convertir en jours (as.integer(date) va donner un nombre de jours depuis le 1er janvier 1970) et ensuite on peut travailler dessus comme sur des entiers :\n\ndates |> as_arrow_table() |>\n  mutate(\n    annee_mise_en_cir = year(date_mise_en_cir),\n    age = (as.integer(as_date(paste0(year, \"-01-01\"))) - as.integer(date_mise_en_cir)) / 365.25\n  ) |>\n  collect()\n\n# A tibble: 2 × 4\n  date_mise_en_cir  year annee_mise_en_cir   age\n  <date>           <int>             <int> <dbl>\n1 2018-04-02        2022              2018  3.75\n2 2020-11-29        2023              2020  2.09\n\n\n\n\n\nSur le second point, on peut remplacer cut par un case_when, soit :\n\npoids |>\n  mutate(\n    poids_vide_tr = cut(poids_vide,\n                      breaks=c(0,1000,1200,1500,1800,2000,Inf),\n                      include.lowest=TRUE,\n                      labels=c(\"1 - moins de 1000 kg\",\n                               \"2 - plus de 1 à 1,2 t\",\n                               \"3 - plus de 1,2 à 1,5 t\",\n                               \"4 - plus de 1,5 à 1,8 t\",\n                               \"5 - plus de 1,8 à 2 t\",\n                               \"6 - plus de 2 t\")))\n\n# A tibble: 4 × 2\n  poids_vide poids_vide_tr          \n       <dbl> <fct>                  \n1         10 1 - moins de 1000 kg   \n2       3000 6 - plus de 2 t        \n3       1700 4 - plus de 1,5 à 1,8 t\n4       1300 3 - plus de 1,2 à 1,5 t\n\n\nPar :\n\npoids |>\n  as_arrow_table() |>\n  mutate(\n    poids_vide_tr = case_when(\n      poids_vide <= 1000 ~ \"1 - moins de 1000 kg\",\n      poids_vide <= 1200 ~ \"2 - plus de 1 à 1,2 t\",\n      poids_vide <= 1500 ~ \"3 - plus de 1,2 à 1,5 t\",\n      poids_vide <= 1800 ~ \"4 - plus de 1,5 à 1,8 t\",\n      poids_vide <= 2000 ~ \"5 - plus de 1,8 à 2 t\",\n      poids_vide > 2000 ~ \"6 - plus de 2 t\")) |>\n  collect()\n\n# A tibble: 4 × 2\n  poids_vide poids_vide_tr          \n       <dbl> <chr>                  \n1         10 1 - moins de 1000 kg   \n2       3000 6 - plus de 2 t        \n3       1700 4 - plus de 1,5 à 1,8 t\n4       1300 3 - plus de 1,2 à 1,5 t\n\n\nÀ noter que :\n\nla version arrow::case_when() ne supporte pas l’argument .default de dplyr::case_when()\ndbplyr supporte également cette syntaxe et que la traduction SQL est la même que celle de base::cut())\nje trouve la syntaxe case_when() plus explicite, les limites sont en face du texte correspondant\n\nJe laisse au lecteur le soin de convertir l’ensemble du script.\n\n\n\n\n\n\nNote\n\n\n\nEn utilisant write_dataset() pour écrire les données, le traitement met 80 secondes et utilise 15Go de mémoire."
  },
  {
    "objectID": "docs/cas-pratique-traitement-duckdb.html#conclusions",
    "href": "docs/cas-pratique-traitement-duckdb.html#conclusions",
    "title": "Cas pratique d’une migration d’un traitement vers duckdb",
    "section": "Conclusions",
    "text": "Conclusions\nNous avons réussi à diviser d’un facteur 10 le temps de traitement et la mémoire utilisée avec un investissement en temps raisonnable et surtout en rendant le code beaucoup plus lisible. En passant, il peut être intéressant de passer par des fonctions duckdb.\nIl est même tout à fait envisageable dans le cas présent et avec les deux versions duckdb de traiter l’ensemble des fichiers en une seule passe. Le test montre que, en augmentant la limite de mémoire allouée à duckdb à 15Go, ce qui reste très raisonnable, les deux méthodes permettent de traiter d’un bloc l’ensemble des fichiers en 12min en utilisant autour de 12Go de mémoire.\nLa version “full” arrow met à peu près le même temps mais consomme 150Go de mémoire en pic !\nEt pour finir : le traitement utilisant uniquement duckdb permet de mieux maîtriser la consommation mémoire au prix d’un ralentissement important quand il arrive à la limite fixée. En fixant une limite à 10Go à duckdb pour traiter l’ensemble des fichiers en une seule passe, le temps de traitement passe à 45 min soit 3 fois le temps qu’il faudrait pour traiter les fichiers séparément (duckdb écrit des données temporaires sur disque, ce qui est beaucoup plus lent que de travailler en mémoire). Pour traiter des fichiers particulièrement volumineux, il peut dont être intéressant de continuer à séparer les calculs.\n\n\n\n\n\n\nQuand vos données sont volumineuses\n\n\n\n\nutilisez tbl() ou arrow::open_dataset() plutôt que arrow::read_parquet()\nutilisez duckdb/dbplyr ou arrow plutôt que dplyr\nquand vous avez des jointures sur des grosses tables, utilisez duckdb/dbplyr plutôt que arrow\nutilisez arrow::write_dataset() ou COPY ... TO ... plutôt arrow::write_parquet()\nsi ces outils repoussent (beaucoup) les limites, ils ne sont pas magiques : il peut encore être intéressant de scinder des calculs en plusieurs blocs pour les accélérer voire les rendre possibles."
  }
]